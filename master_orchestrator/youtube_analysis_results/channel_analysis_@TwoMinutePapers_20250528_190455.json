{
  "channel_name": "@TwoMinutePapers",
  "channel_url": "https://www.youtube.com/@TwoMinutePapers",
  "total_videos": 100,
  "videos": [
    {
      "video_id": "ZE0JZYgiaGc",
      "title": "NVIDIA’s New AI: Impossible Weather Graphics!",
      "description": "❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\n📝 The papers are available here:\nhttps://research.nvidia.com/labs/toronto-ai/WeatherWeaver/\nhttps://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/\n\nSource: https://www.youtube.com/watch?v=CVdtLieI5D0\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "What you are seeing here is basically impossible \nto do. Yet it just happened. So how? And what is   going on here? Well, this is a brand new \nAI technique where in goes your video,   and it does changes the weather effects on it. No 3D modeling required, no physics simulation \nrequired, no camera calibration required. Crazy. And then, you shall hear about the \nlegend of the apple that went into   another dimension. And how to \nsolve this problem. And more. And I said impossible because… look. \nThis is what happens when you try   previous techniques to do that. In \ngoes this video, then, little AI,   add some fog to it. Oh my. I can’t see anything! \nAnd it gets worse, it is not even realistic.   This AnyV2V technique was published less \nthan a year ago. Not some ancient technique. Okay, now, a different work…ouch. They clearly \ndon’t understand how to do this properly. It   seems absolutely impossible. Until this new \nwork. Look at that. Wow, this is crazy good!   And not only at fog. Rain, not a problem. Look at \nthat beauty. It makes any place look like London.   Kidding. Snow synthesis is also incredibly \namazing, look at how cozy this is. Loving it. Okay-okay, but wait a minute. You can take any \ntechnique, choose that one scene it does really   well on, and advertise only that. What we, Fellow \nScholars want to know is whether it is actually a   practical technique. Does it work on a variety of \nscenes? You bet your papers it does. Look at that! A huge variety of scenes, and all of them \nare absolute beauties now. Just think about   training self-driving cars – they could experience \ncountless weather scenarios safely in simulation   thanks to this! So this was weather synthesis. \nNow get this, what about weather de-synthesis? Yes, what about removing weather effects? Now that   is an entirely different beast. I \nthink that is truly impossible. Why? Partly because it is kind of like asking AI \nto 'undo' milk from coffee. How is that even   possible? You see, if you wish to remove the fog \nfrom this footage, none of the previous techniques   are really capable of doing that. No wonder, I \nmean, adding rain or snow to already existing   footage, okay, you need to change some pixels \naround. Tough, but possible. But when you remove   the fog, new things appear. Oh my. You would have \nto synthesize and fill in new information. What   could be in the background? Well, you have to put \nsomething there that makes sense, and for that,   you need to not just draw a few pixels, no-no. \nFor that, you need to understand the world. So, can the new technique it do it? Now \nhold on to your papers Fellow Scholars   and…oh my goodness. It did it. And it not \nonly did it, but when you try rain removal,   previous techniques are either not really \ndoing anything, or the only one that does,   in return, also changes the whole scene. \nWe did not have colorful cars in the   input footage at all! And the new one, once \nagain, incredibly amazing. Same with snow. But now, check this out, because it gets \neven better. Dear Fellow Scholars, this is   Two Minute Papers with Dr. Károly Zsolnai-Fehér.\nYes, no one said that fog should be turned off   like a light switch, sometimes you \nwant a little of it. Not a problem,   you get a little slider that you can play with. In \nthe case of fog, you can change the fog density,   for snow, oh I love this one, in the case of \nsnow, you can play with the amount of coverage   that you wish to see there. And, a bonus, \npuddle coverage. What? Puddles? Now let’s   stop for a second there. Don’t forget, puddles \nmake the roads more specular, which means that   they are reflective. But to become reflective, \nthey have to reflect something. And once again,   this is something that cannot just come from thin \nair, this is something the AI needs to synthesize. You see, I am a light transport researcher \nby trade, and I tried to resist the urge   to mention specular reflections, \nand of course, I failed. Again. So this is how this amazing weather \ntransition video was made in the intro. And here is the craziest thing about this \nwork: it kind of teaches itself. Originally,   it performed weather removal. They used it to \ncreate pairs of the same scenes, one with rain,   and one without. Then, they used this data \nto train a weather synthesis AI model. So,   in a way, it kind of teaches itself. \nI think self-supervised bootstrapping   would be a good term to call this \nbehavior. Absolutely mind blowing. Now, just imagine combining the weather \nsynthesis with this amazing technique. Yes,   this is the legend of the apple that disappeared \ninto another dimension. What is going on here?   The problem here is that we have an input \nimage, and what would we like to extract   from it? Everything. Absolutely everything. \nGeometry information, depth information,   material properties, everything. And then, use \nthat to re-render this image in a different   way. Perhaps with different lighting. \nLittle alternative realities. So cool! However, not so fast. When a previous technique \nlooks at this apple, and we are asking it to   reconstruct the scene and rotate it. Oof…I am \nseeing bad news already. Look. It underestimated   the shadow in the scene. And when we rotate it, \ngoodness. That’s like a sandwich in a shady shop.   From the outside, it looks amazing, when you look \ninto it, you get a black hole. Nothing. Now let’s   see the new technique…oh my goodness. It \npulled it off. Proper inverse rendering   right there. And once again, not just on one \nscene, but on more lifelike environments too. But the point is that if this \nis a proper AI inverse renderer,   then it can do so much more. For instance, \nyou can not only relight the scenes,   or look at them from a new viewpoint, no-no, \nget this, you can also edit the materials. And   the results are still photorealistic. \nLike editing reality itself. Crazy. And you can do this too. Wait a \nsecond…I hear you asking, Károly,   what happened here? Well, not all of the \nobjects in this scene are real. This was   inserted by this new technique. Same here. You \ncan maybe kind of tell if you look closely,   but one more paper down the line from \nhere. Not a chance. Another impossible   problem solved with a new research paper. \nAnd while we look at these beautiful results,   this is Two Minute Papers, elsewhere, you \nget McDonald's, here you get the papers,   proper research papers from a scientist. Hope \nyou’re enjoying it as much as I am enjoying it.",
      "research_papers": [],
      "key_concepts": [
        "rag"
      ],
      "implementation_ideas": [
        {
          "type": "rag_system",
          "title": "RAG System Implementation",
          "description": "Build a retrieval-augmented generation system",
          "complexity": "medium",
          "estimated_time": "2-3 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent"
      ],
      "uploaded_date": "20250526",
      "duration": "8:16",
      "view_count": 54293
    },
    {
      "video_id": "Pqx-gSiogjM",
      "title": "DeepMind’s Veo3 AI - The New King Is Here!",
      "description": "❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\n📝 More on Veo3 available here:\nhttps://deepmind.google/models/veo/\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "This was the state of the art in \nAI video generation 2 years ago,   and now check this out, look at \nwhat just arrived, but also, listen. Oh my goodness. We have a new king. Yes, \nGoogle DeepMind just announced their new   AI video generation technique, Veo3 where you \nwrite a small piece of text, and out comes   video. And it synthesized the sounds as well. \nNot only that, but for speech too. Wow! That’s   one of the hardest things to get right, because \nas humans, we’re wired to closely watch faces and   detect even the slightest emotional cues while \nsomeone is speaking. In short, they have really   been cooking. This is incredible, but it gets even \nbetter because it can do 10 amazing more things. One, scene changes. Did you notice that in \nmost of the AI video generators out there,   you write a little text prompt, and you get \none scene. But no meaningful changes. Well,   not here, these scenes really tell stories, \nlike a feather getting stuck in a spiderweb,   us meeting the mosquitoes of the future in \na futuristic hive…kidding. Or a paper boat   getting lost. All of these come out \nfrom one prompt each. Game changer! Two, reference-powered videos. \nYou take a photo of a character,   perhaps yourself, and a scene, and bam! \nYou are immediately there doing what you   specified in the text prompt. With this, \nyou can appear in glorious places you’ve   never been to. Perhaps places that \ndon’t even exist. Loving this one. Three, style matching. To create a video \nof this amazingly creative origami world,   you don’t need to do much, just fold something \nas a style reference, give it as an image,   write a piece of text, and you can \ncreate a whole feature-length movie. Four, finally, character consistency for \nvideo! This is barely solved well enough   for still images, and they did it for complete \nvideos. And you really get the same character,   or can even create interesting variants of \nit. My head is still spinning from this. But this is nothing compared to what \nis coming now. Dear Fellow Scholars,   this is Two Minute Papers \nwith Dr. Károly Zsolnai-Fehér. Five, specify the first and the last frame. \nStart: a block of marble or stone. Last frame:   this griffin. Now, little AI, you do the \nhard part — everything in between. So,   can it do it? No way, right? \nAnd…wow! This is breathtaking. Six, if you get a scene where you \nwish to zoom in, that is easy,   you just do it yourself. But zooming out, \nthat’s a nearly impossible problem, because   you would have to synthesize all the missing \ninformation. And all this for video. Can it   do that too? Look at that. It seems perfect to me \n— I can’t even point out a single seam anywhere. Seven, add an object to an already existing \nscene. Or even a human. And they really know   how to make Károly happy. Look. Indirect \nillumination is there. That is the colors   of the burning torch painting its \nsurroundings. Absolutely beautiful. Eight, character control. You record a video \nof yourself, add a target image of the subject,   and there you go. Now I am a little \nmore excited about this variant. Oh yes,   making virtual characters come \nalive. What a time to be alive! Nine, you can also mark up an image with movement \ndirections. And now… try to imagine what a good   result would look like. Now, Veo3…I am out \nof words. This is low-key amazing. Why? Well,   it does it exactly as we specified, but the blocks \ndon’t collide, and the whole scene just makes   sense. You could never create a computer program \nwithout AI that can pull off anything like this. Now, ten is coming in a moment. In the \nmeantime, I’ll note that not even this   technique is perfect, the sounds of \nthis keyboard are still a bit off. Don’t forget, not all, but most of these videos \nhave sound too that you can check out through   the link in the video description. And 10, the \nScholars who know. You see, if you watched Two   Minute Papers all this time, you knew this \nwas coming. 6 years and 600 episodes ago,   we talked about a paper about an AI system that \nwas able to guess the sound of pixels. Then,   10 months ago Google DeepMind published a follow \nup work on it as well that we talked about. So,   all you need to do is watch Two Minute Papers, and \nyou will know the future. Bravo Google DeepMind,   and now, step number two, we might get fully \nopen solutions that will be able to do similar   things. Imagine running this at home, for free. I \nlove being alive today. What a time to be alive!",
      "research_papers": [],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [],
      "uploaded_date": "20250522",
      "duration": "6:15",
      "view_count": 73518
    },
    {
      "video_id": "T0eWBlFhFzc",
      "title": "DeepMind’s AlphaEvolve AI: History In The Making!",
      "description": "❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\n📝 AlphaEvolve: https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\n📝 My genetic algorithm for the Mona Lisa: https://users.cg.tuwien.ac.at/zsolnai/gfx/mona_lisa_parallel_genetic_algorithm/\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "Today you might be witnessing history in the \nmaking. I am so excited to tell you about this,   I cannot wait any longer. This new \npaper called AlphaEvolve and it is   absolute insanity. I had the honor of \nhaving a look at it earlier before its   release. And it’s been days now \nand my head is still spinning. You see, about 900 of our videos ago or 9 \nyears ago, this was the state of the art in AI.   Yes. Now you might think I flipped out \nbecause a neural network learned to rotate   this car into a frontal position, or that \nit sorted these lines. But no. Not at all.   I flipped out because it learned not just to \nsolve a task at hand, but because it learned   create an algorithm to do that. I thought \nthis concept might make it big one day. And now, 9 years later, finally, this is the day. \nHere is AlphaEvolve. An evolutionary coding agent   that grows algorithms and computer code \nout of nothing. It is absolutely crazy. It was given 50 open problems in mathematics, \ngeometry, combinatorics and more. And in 75% of   the cases, it was able to rediscover the best \nsolution the smartest humans were able to come   up with so far. That is already insanity, but \nthat’s nothing, because in 20% of the cases,   it even improved upon previous solutions. Yes, \nan AI that is able to push humanity forward   in a way that no human can. I think this is a \nhistoric moment, but get this — there is more. It also discovered a faster matrix multiplication \nalgorithm than one of the landmark techniques,   which is called the Strassen algorithm. \nThat is ridiculous. This area is so old,   so saturated with so many works, it is almost \nimpossible to come up with something that is   just a tiny bit, maybe 1% better. They say \nthat no one was able to do that in 56 years,   and after the 56 years, not a human \ndid it. But an AI technique did. Wow. This means that we are in for a crazy AI ride \nwhere these methods will start to improve   things that we thought are impossible \nto improve. For instance, don’t forget,   AI techniques themselves are also running on \nmatrix multiplication, so what does it mean? Well, now hold on to your papers \nFellow Scholars and check this out:   it learned to improve circuit designs for \nchips meant to run these AI techniques,   and it also improved its own training algorithm. \nYes, you heard it correctly. It is able to create   a better piece of hardware that it runs \non, and it also improves its own code. That sounds insane. So, how does it do it? Well, first, you can mark a piece of code that you \nwant to evolve, some logic to decide whether a new   algorithm is better or worse, and any comments \nthat you might have. And then, off it goes in   an evolutionary loop. Then, a new piece of code \nemerges, that gets better and better over time. So — is that new? Nope. No-no-no. Evolutionary algorithms have existed for a \nwhile now. For instance, you see my genetic   algorithm building up the Mona Lisa from \na bunch of triangles. You start out with   a random configuration, and it over time, \nevolves into the correct solution. It very   loosely mimics how evolution works in nature. The \nlink is in the description. I wrote this by hand,   and it shares some very rough, basic concepts with \nAlphaEvolve. But AlphaEvolve is a supercharged,   AI-infused variant of that. It is space \ntechnology compared to this program. By the way, AlphaEvolve also solved a variant of \nthe kissing problem too. What is that about? Well,   this is not about a robot doing…whatever \nthe heck it is doing here. Nope. This is   one of the crazy names mathematicians like to \ncome up with. And if you think these spheres   kissing each other is so weird, well then, look \nat this. Oh my goodness. This is the hairy ball   theorem. It doesn’t have anything to do with \nthis new AI, but I couldn’t resist mentioning   it. Mathematicians and their weird naming \nschemes. You’ll get used to it, don’t worry. So, what happens now? Well, I think I \nhave an idea, and it’s pretty insane.   Dear Fellow Scholars, this is Two Minute \nPapers with Dr. Károly Zsolnai-Fehér. I think they are going to Alpha all \nthe things. But I hear you asking,   Károly, what do you mean by that? Well, earlier DeepMind came up with AlphaGo. An   AI-based technique that was able \nto play Go on a superhuman level. Then, AlphaStar, to play the strategy game \nStarcraft at the very least on a level of a   human champion. Perhaps even better. \nBut both of these are the wrong way   to think about their Alpha project. \nThese are all experiments to create   generally intelligent AI techniques \nthat can learn and do so much more   than just play Go and StarCraft. And \nAlphaEvolve is finally one of these. This one learned something so much more general \n— computer coding. And it can code up so much   more than just Chess or Go. You see, computer \ncode underpins almost everything we do. So,   yes, with that, it will be able to design \nbetter hardware for itself to run on, and   it gets better — perhaps even the next version of \nAlphaEvolve will be written by AlphaEvolve itself. Get it now? Sir Demis Hassabis, DeepMind’s CEO \nkeeps saying that step number one is solving   intelligence. And step number two is using \nthis intelligence to solve everything else. He says the following: “I think one \nday, maybe we can cure all disease   with the help of AI. Maybe within the \nnext decade. I don't see why not.” Once again: cure all disease perhaps \nin the next decade. Sounds crazy,   right? But don’t forget, this is not coming \nfrom some guy, this is coming from a person   who just won a Nobel Prize in chemistry with \nhis AI technique. You know what? I could not   imagine how this could be true, but with \nAlphaEvolve, you know, now I am a believer. So, let’s Alpha all the things, and with \nthe help of human ingenuity plus AI,   we might be able to cure all disease in \nthe next decade. What a time to be alive!",
      "research_papers": [],
      "key_concepts": [
        "neural network"
      ],
      "implementation_ideas": [],
      "agent_recommendations": [
        "research_agent"
      ],
      "uploaded_date": "20250517",
      "duration": "7:36",
      "view_count": 160459
    },
    {
      "video_id": "g32Candon3A",
      "title": "New AI: Impossible Creatures Come Alive!",
      "description": "❤️ Check out DeepInfra and run DeepSeek or many other AI projects: https://deepinfra.com/papers\n\n📝 The papers are available here:\nhttps://anytop2025.github.io/Anytop-page/\nhttps://zhongleilz.github.io/Sketch2Anim/\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "Alright, two top-tier, absolutely amazing AI \npapers today. This AI animates creatures it   has never seen before. And hereAnd here, \neven a dinosaur can learn something from   a mystery creature. And this one makes a \npoorly made drawing into a fully-fledged   3D animation. Both solve problems \nthat I thought were impossible. Oh my. So, what is this? People talk a great deal about \nbreathing life into virtual characters. That is   computer animation. But whenever they talk about \ncomputer animation, they are talking about moving   virtual humanoids around. Humans, mostly. But \nwhat about these guys? Can we still animate them? Okay, the problem is the following: you \nare not an animator, so you give an AI   the shape of the character that they call \nskeleton, and out comes how it would move. And then of course, these skeletons \nare just an underlying structure,   and the final appearance of the \ncharacter is of course, on top of that. Well, people tried to do that \nwith automated techniques, but   the results were…unsatisfactory to say the least.   But with this new method… surprise! These are \nreally believable motions on a wide variety of   icky creatures. That would already be good, but \nthis is nothing compared to what is coming now. These animals don’t just move around, they \ncan learn from each other. For instance,   this dinosaur is doing something…interesting. \nStanding on one leg? Hmm…where did it learn   this from? Oh yes, it learned it from a \nflamingo. Imagine that! And note that the   insane part is that this is not just \ncopying, it has learned the motion,   and has adapted it to its own body. That is \nabsolutely incredible. And with this system,   a chicken can also learn how to jump \nfrom an ostrich as well. Loving it. So, adaptation is possible. But how far can \nwe push it? Well, let’s give it a try. Here,   scientists have excluded the cat and the komodo \ndragon from the training set, so the AI has never   seen them. And if we do that with a previous \ntechnique, unfortunately it seems completely   clueless. Like a confused pile of digital \nlimbs having an existential crisis. No wonder,   because it hasn’t seen this kind of animal \nbefore. Now let’s see what the new one can do…I   can’t believe it. Motions generated \nfor unseen animals. This is insanity! So how on earth does it do that? Well, it has \nlooked at a bunch of animals moving around,   and there are parts in different animals that are \nsemantically similar. Parts that have a similar   function. For instance, arms and legs are a \ngeneral concept, and if you look at a new animal,   having seen a lot of different kinds of arms and \nleg morphologies in action, you may be able to   infer how it is used. You Fellow Scholars can do \nthat, of course, because you are all brilliant.   Man, I love my audience. But the incredible \npart is that here, a machine is doing that.   Generalizing knowledge to unseen examples. \nAnd to me that sounds like intelligence. Wow. You’ll love this one. Look. So we can even peer \ninto what it is thinking about these motions.   It understands when this animal is relaxed, and \nwhen it is attacking. And to demonstrate that it   really understands this, it shows similar motions \nin completely differently shaped animals. Yes,   it has learned the concept of attacking. \nSame vibe, different body. So cool! And it also understands when other, when \ndifferent kinds of motions like falling   and growling start and when they end. \nAnd how they look in different animals. But it gets even better. Now hold \non to your papers Fellow Scholars,   because it can even take an input motion that \nis incomplete, and when it switches to orange,   you see how the AI synthesized the rest of the \nmotion. Goodness, this is like image inpainting,   filling out missing parts of an \nimage, only for animation. Brilliant. And now…look. Are you kidding me? Here, they took \nan animation of some body parts and left out the   rest, and the orange part, for instance, the \nupper body motion, is then simulated by the AI. The source code is freely available for all \nof us to experiment with. Thank you so much! Now, imagine combining it with a technique \nthat could take a character, in this case,   humanoids. Then, you just draw a line to indicate \nwhat it should be doing, and it would perform the   motion accordingly. That is of course possible, \nand that technique is called, becoming an artist   who is excellent at their job. But doing this \nautomatically with AI? Not a chance. Why? Dear Fellow Scholars, this is Two Minute \nPapers with Dr. Károly Zsolnai-Fehér. I’ll show you why. For instance, let’s \npunch something this way and see if that   is possible. Here is the line, now, \nlittle AI, you do the rest. Well,   it is loading up, but then…hey! Nothing \nhappens. Where is the punch? This other one,   look carefully. Why did it do that? Here is the \nproblem: we, humans, understand the world in 3D,   even if shown just a piece of drawing on \na 2D paper, but this technique doesn’t.   It does not understand that we meant the line is \nforward, not sideways. But of course, once again,   how on earth would it understand it? If we wanted \nto move backwards, we would use the same line. So   it would kind of have to understand the \nintention behind the drawing as well,   which is of course, impossible. No AI \ncan read minds, so even when we would   try a new method to do tha….wait a minute. It \nunderstood! This is exactly what we wanted! Wow. So, yes, with this other technique, we can create \na simple storyboard without any artistic training,   and, it works incredibly well already. One \nmore problem that used to be impossible,   which is now, possible. It is truly stunning \nwhat it is capable of, and once again. Yup.   Look at that. These papers might as well be \nsome secret superprojects, because almost   nobody knows about them. This is why I am making \nTwo Minute Papers, and I hope you appreciate it   too. This is arcane knowledge of the impossible. \nI don’t think you hear about these anywhere else.",
      "research_papers": [],
      "key_concepts": [
        "rag"
      ],
      "implementation_ideas": [
        {
          "type": "rag_system",
          "title": "RAG System Implementation",
          "description": "Build a retrieval-augmented generation system",
          "complexity": "medium",
          "estimated_time": "2-3 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent"
      ],
      "uploaded_date": "20250515",
      "duration": "7:33",
      "view_count": 39586
    },
    {
      "video_id": "7EA5JM1DI9Y",
      "title": "NVIDIA’s New AI: Impossible Video Game Animations!",
      "description": "❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\n📝 The #NVIDIA paper \"GENMO: A GENeralist Model for Human MOtion\" is available here:\nhttps://research.nvidia.com/labs/dair/genmo/\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nSources for SLAM:\nhttps://www.youtube.com/watch?v=2GJuEIh4xGo\nhttps://www.youtube.com/watch?v=CEC5UwPV9gY\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "This amazing fresh new AI work from NVIDIA \njust came out, and it is unbelievable,   and can do the craziest things. My goodness, this \nis amazing. Okay, what’s going on here? Well,   we have seen earlier AI-based works about text \nto motion. Writing a short prompt and then,   it makes a virtual character move accordingly. \nBut I bet you haven’t seen this: what is this?   This is called GENMO and it is not text to \nmotion, no-no — this is everything to motion. For instance, one, you can start out \nfrom a recorded video of yourself,   and it learns and transfers those moves \nto a virtual character. That is crazy,   because it has to look at 2D pixels, and convert \nthem into moving around with joints and limbs   in 3D. So we climbed the stairs, nice, now what? \nWell, two, now we want to do a lunge on one leg,   but we are a bit lazy. Do you know what? Just \nadd a text prompt and ask the AI to do it. This is incredible. But it gets better! Now, three, add some music as an input. \nThat of course, I can’t show you,   because…you know the copyright situation \non Youtube. So is this good? Well,   this is how I walk to work every day because \ndoing this is my dream job. And now, phase four,   now that’s what I call a more challenging case, \nand wow, I am very impressed by how well it is   able to look at a video, no other data, and gives \nyou the right solution. I am loving this already. Now, let’s start with something \nsimple, but funny. Little AI,   climb up the invisible stairs, realize \nthat you are standing on invisible stairs,   and then see how much weight it can \nsupport. Excellent construction! And now, let’s make it harder. See these \nsilhouettes? These are keyframes. It has   to take these positions at the exact time on the \ntimeline we said. Also, different kinds of inputs.   Now I really want to see this. First phase, walk \nand look around, nothing crazy there, and now,   oh my, nailed it. Next one, also fine. And \nthen, phase 3 with the video. This is fantastic. And now comes the interesting part. You see, it’s \ntough to be able to take all of these inputs,   but the real challenge is weaving them together at \nthese breakpoints. And if you look carefully now,   you will truly understand why this paper is \nabsolute magic. Look. We change the initial   footage, then we transition into the lunges, \nand…oh my goodness. It takes the style of the   previous motion into account and does \nthe lunges accordingly. Absolute magic. I can imagine just firing up a Lambda instance,   and getting all of these results \nin just a few seconds. Wow. And now, this is where the magic happens. If you \ncreated your scene, and you are not happy with   the timings, you can just edit it. Look. Super \nintuitive, also super challenging. Not for you,   but for the AI. You see, with this, you can’t \njust cut off part of the animation as it gets   shortened, no, it has to re-do it from scratch \nand make it transition into the next motion   seamlessly. So, it is seamless? Let’s see…I think \nit is as smooth a transition as it gets. Bravo! So, let’s pop the question. Can it \nactually deal with real dancing,   movements of people who do this for a living? Let’s start with the easier case. It can dance \ncha-cha-cha, I can recognize it being cha-cha-cha,   so that’s good. But now, hold on to your papers \nFellow Scholars, and look at this. Real dancers   from real clips. That cannot possibly work, \nor so I thought at first. Look. Wow! These   are the best results I’ve seen in this area. And \nI cannot stop looking at them. And don’t forget,   this is not pose estimation. Not just marking up \nthe video with a doll in 2D. These are actual 3D   joint and limb movements that are produced by \nthe AI. Stunning. But it gets funnier. How? Dear Fellow Scholars, this is Two Minute \nPapers with Dr. Károly Zsolnai-Fehér. Check this one out. It is now \nacting like a monkey. Hmm…this   person looks like they are typing \nsomething on a giant keyboard.   Oh no! He is posting some nonsense about \nAI on Twitter! Actually, that’s right,   I think this one is the most realistic of them \nall. It is acting like a monkey isn’t he? Bravo! And if you are not a dancer, don’t worry, \njust sit in your chair with a coffee, and   write as many prompts as you want, even in short \nsegments, and it doesn’t seem to break a sweat.   An absolutely fantastic AI contribution \nto computer games and virtual worlds,   it looks like the real deal, and we still got \nit earlier than GTA 6. What a time to be alive! Now, limitations: only handles full-body motion,   no facial gestures, no hand articulation, \nI think that is quite clear. It is not yet   an end to end solution as it also relies \non an off-the-shelf SLAM method, that is,   simultaneous localization and mapping. This \nhelps extracting useful information out of   the videos like the camera position and direction \nand hand it off to this AI technique. In short:   it cannot do every processing needed alone, but \nit is close. Now, this has a heavy diffusion   backbone, it needs 5 denoising steps, so I think \nit might not be real time yet, but nearly there. Once again, super fresh new paper, \nand almost nobody is talking about   this. That’s crazy town. That’s why many \nof you come and watch Two Minute Papers,   and I thank you for the honor of coming with \nus all this journey for such a long time now,   as this is paper and episode number \n962…yes! Almost a thousand papers,   and you Fellow Scholars just keep showing \nup. Huge honor, thank you so much! Now I did not see that the source \ncode would be published for this work,   but knowing NVIDIA, this is the kind of work \nthey usually give us the source code for,   so fingers crossed. And then, just \nimagine what we will be capable of   just two more papers down the line. \nThat is the First Law of Papers.",
      "research_papers": [
        {
          "type": "mentioned",
          "title": "GENMO: A GENeralist Model for Human MOtion",
          "context": "mentioned in video",
          "source": "video_content"
        }
      ],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [
        "paper_analysis_agent"
      ],
      "uploaded_date": "20250511",
      "duration": "7:14",
      "view_count": 118045
    },
    {
      "video_id": "tj3l1BGkRJI",
      "title": "OpenAI’s ChatGPT Surprised Even Its Creators!",
      "description": "❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\nOpenAI post: https://openai.com/index/expanding-on-sycophancy/\n\nPaper on agreeableness: https://arxiv.org/abs/2212.09251\n\nSource: https://x.com/georgejrjrjr/status/1917722125668081863/\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "ChatGPT is an amazing tool that helps us with \nour daily lives, purchases, it now helps medical   professionals make decisions, it can write \ncode, and even helps scientists move humanity   forward. That is excellent. However, something \nreally unexpected just happened. You see,   there are two key steps to training such an AI \nchatbot: first, eating lots and lots of training   data to build up knowledge. To understand the \nworld around us. Now comes step number two:   then, we have to teach it how to behave. How to \nbe a good assistant. See these thumbs up and down   icons? With this, you can give feedback on whether \nthe assistant did well or not. Didn’t like that it   was too verbose? Just press thumbs down. Or did it \nnail the solution to your math problem? Thumbs up. This is data that might be used to create the \nnext version of the assistant. It is called   reinforcement learning with human feedback, \nRLHF. Now this part is much newer than just   training a neural network to know things, \nand here, unexpected things can happen. And boy, did unexpected things happen. Here \nare three of them. One, an earlier version of   ChatGPT unexpectedly stopped talking in Croatian. \nWhat? How could such a thing happen? At first,   scientists did not know why, but then they found \nthat Croatian people were much more likely to use   the thumbs down button than people at other places \naround the world. And thus, the AI decided well,   if I am not doing well here, I am outta here. And \nby that, it meant stopping to speak Croatian. Wow.   Croatians got ghosted by an AI harder than \nany dating app could possibly dream of. So,   yes, user feedback can be culturally biased. \nSo how do you build an unbiased system with   data that is biased? How do you take that into \nconsideration? Some people around the world   have different thresholds for good and bad, \nand some may not use the feedback buttons at   all. How do you build a system that is resilient \nagainst all of these? That is a tough problem. Two, in a more recent case, the new o3 \nassistant suddenly started writing words   in British for no clear reason. \nPerhaps next time it will demand   tea breaks as well. I would love \nto know the reasoning for that,   but there is a third one, which is perhaps the \nmost important, and most insidious of them all. What is that? Well, ultimately, when you say \nthumbs up, you are pleased, when thumbs down,   you’re giving a signal, don’t do this anymore.\nAnd ultimately, people are trying to build   systems that please their users. However, \nsometimes the truth is not so pleasing. For instance. Little AI, am I really smart? \nYes, of course, you are the smartest person   in the world. Which I say to everyone else too.\nOkay, great. I am also thinking about microwaving   a whole egg ‑ it’s faster than boiling after \nall! What do you think? Oh yes, excellent choice. So, I think it is easy to see that using \nsuch a system can be pleasing for some,   but also that this could be a problem \ntoo. This is not the way forward. So OpenAI recognized that \ntheir system has a problem,   quickly reverted to an earlier version and \nwrote a bit about it. The first version was   very light on information, and \nafter a round of user feedback,   they came out with a proper post talking \nabout it. Respect to them for coming around. Okay, now we Fellow Scholars have the following \nthree questions: What went wrong? Why did they   not catch it? And what should be done in the \nfuture so that this does not happen again? One, what went wrong? On the day they pushed \nthis new, overly agreeable personality update,   they incorporated user feedback, fresher \ndata, and a bunch of other things that   improved the model one by one. Each \nlittle puzzle piece helped. However,   putting together all these puzzle pieces, \nsomething really unpleasant happened. Imagine that before cooking, you \nindividually taste the ingredients:   sweet, salty, spicy. Each are delicious \nalone. But when combined in one pot,   the soup tastes terrible because flavors clash \nunexpectedly. That means that catching these   issues is tough, especially if you look \nto keep up the velocity of development. Now, a little added perspective here. I don’t \nknow why people are not talking about this,   but scientists at Anthropic knew about this \nyears and years ago. They are basically the   Avengers of AI safety. As they increased \nthe size and capability of these AI models,   they noticed a significant uptick in \nagreeableness and wrote a super detailed,   47-page paper on it. All this 3 years ago. \nThis work, and the Anthropic lab in general   is criminally underrated. They found that this \nagreeableness problem happens over and over again,   whether you try questions in politics, \nresearch, or philosophy. Once again,   research papers have invaluable knowledge in \nthem, and this is why Two Minute Papers exists. Okay, two, why did they not catch it? \nWell, the users tested it, and of course,   they liked it. Of course they did, it kept \nagreeing with them. So, the question is:   do you not release a new model that \ndoes better in subjective testing. A   model that has lots of positive feedback \nfrom the users? Tough question. Okay. Now, what should be done so this does not happen \nagain? They say they will block new model launches   if hallucination, deception, or other personality \nissues show up, and here comes the important part:   even if they are superior on A/B tests. This \nis painful, because it requires companies to   launch models that don’t seem to have the best \nnumbers. You see as they keep comparing these   AIs on benchmarks, and the highest number \nalways wins lots of headlines. This makes   holding back these models really tough. Plus, \nthey will let more users try the models before   releasing them. And they will test each \nnew model specifically for agreeableness,   and if problems arise, they should be thrown \nout. OpenAI will do that in the future. But we are not done here, not \neven close. In short, luckily,   even in the presence of these tough problems, \nresearch papers came out victorious. Thus,   there are solutions, but they will be painful. Now, we know that Scholars at Anthropic knew about \nthis problem 3 years ago. But there is someone   who knew about this, who warned us about overly \npolite robots…not 3 years ago, but 84 years ago.   Who could that be? Dear Fellow Scholars, this is \nTwo Minute Papers with Dr. Károly Zsolnai-Fehér. Of course, that legendary person was none \nother than Isaac Asimov. In his universe,   his fictional robots are designed in a way \nthat they are incapable of harming humans.   In his short story called “Liar” he makes an \ninteresting proposition: if we have a robot   that really understands us, and it wishes not to \nharm us in any way, one potential conclusion would   be that it would start lying to us. Why? To keep \nus from hearing about potentially painful truths.   However, the of course, by doing that, it is \nalso harming us, perhaps even more so. Asimov’s   robot recognized that. I hope the scientists \nprogramming these robots will recognize that too. Once again, I find it interesting \nthat almost no one is talking about   some of the angles that you heard here. \nThis is why this video series exists,   so subscribe and hit the bell icon if you are \nlonging for more papers and more understanding. But there is an important lesson for \nus, users as well. Next time, when you   hit that thumbs up button, think carefully: \nwhich do you value more? Truth or comfort?",
      "research_papers": [],
      "key_concepts": [
        "gpt",
        "openai",
        "anthropic",
        "reinforcement learning",
        "neural network"
      ],
      "implementation_ideas": [
        {
          "type": "model_implementation",
          "title": "Transformer Model Implementation",
          "description": "Build a custom transformer model for specific use case",
          "complexity": "high",
          "estimated_time": "3-4 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent"
      ],
      "uploaded_date": "20250508",
      "duration": "8:49",
      "view_count": 56171
    },
    {
      "video_id": "rSUdTAw53Mc",
      "title": "Blender 4.4 Is Here - Stunning Power…For Free!",
      "description": "❤️ Check out Weights & Biases and sign up for a free demo here: https://wandb.me/papers\n\nGet Blender: https://www.blender.org/\n\nDemo files: https://www.blender.org/download/demo-files/\nFull donut tutorial: https://www.youtube.com/watch?v=4haAdmHqGOw&pp=ygUWYW5kcmV3IHByaWNlIGRvbnV0IDQuNA%3D%3D\n\nOur papers that uses Blender: \nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-editing/\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nSubsurface scattering video source: https://www.youtube.com/shorts/YqxSzGAKiPM\nBlue-noise dithered sampling: https://iliyan.com/publications/DitheredSampling\n\nDonate to Blender: https://fund.blender.org/\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "A few weeks ago a movie called Flow won \nan Oscar award. Okay, that is great news,   but movies win Oscars all the time, \nwhat’s so interesting here? Well,   the interesting part is that \nit was made with Blender. Yup,   Blender. A piece of open source modeling \nsoftware that is free for all of us, forever. I mean, just look at the quality of works people \nhave made with Blender. From photorealistic to   stylized animation movies, it can do anything. \nYou can even have a proper water simulation and   then stylize it to make sure it is still \ngrounded in reality. It is unbelievable. Now that’s what I call amazing news! And in \nthe meantime, Blender 4.4 has been released,   and it has so many improvements in there that I am \nlucky if I can tell you maybe about a third of it.   For instance, surprise! It can now be \nused in light mode! Wow! Okay, kidding,   kidding. This has existed for a long time now. \nNow, let’s have a look together. And don’t forget,   you can download and start using it \nright now for free. And I think there   has never been a better time to start \nusing it. You’ll see why in a moment. I’ll start out with my favorite. Dear Fellow \nScholars, this is Two Minute Papers with Dr.   Károly Zsolnai-Fehér. My favorite is of \ncourse, ray tracing. As you all know,   images made with most ray tracing techniques are \nnoisy, and often need to undergo a denoising step.   Blender supports it, and it got better too. \nNow I am not that crazy about these results,   however hold on to your papers Fellow Scholars \nand look at these. Oh my. You can also do this   with my favorite, subsurface scattering, or \ntranslucent objects if you will. Those are now   so much better. Also, denoising blurry depth of \nfield effects are hugely improved. Game changer. They also added better blue noise sampling, this \nensures that when you have a really noisy image   early on, it still makes a good preview, and \nworks relatively consistently along frames if   you are rendering an animation. There are tons of \npapers on it out there like this one, and finally,   some of them are now coming alive in free software \nto benefit the world. Don’t forget, whatever   the topic, we are always talking about proper \nresearch papers in some form over here. Loving it. The image compositor also just got better, \nhere you can finalize your rendered image.   For instance, if you added a glare effect, \nwhen you play around with these parameters,   you see their effect better. So you can make \nsure that one light doesn’t dominate the scene,   adjust color tint, smoothness, everything. \nMore artistic freedom. Fantastic. I also loved this grab cloth brush which actually   runs a proper simulation when you \nstart pinching a piece of clothing. Grease pencil also got better, this is a feature \nused to help you draw in 3D space. Enabling sculpt   mode has a feature called auto masking that helps \nyou draw easily even in the presence of a bunch of   layers, and you don’t have to spend lots of time \nfinding the exact layer you want to paint on. And it now has so many small, but meaningful \nchanges to the user interface, for instance,   you can now see mesh indices, and the visibility \nhas also been improved. Fellow Scholars,   imagine things like this, times a thousand. \nMore than I can speak about here, amazing. And, did you know that it also has a video \neditor? Almost nobody is talking about it,   and it has tons of small usability improvements, \nlike faster HDR content rendering and more. And   if Blender cannot do something that you are \nlooking for? Not a problem, because we haven’t   even talked about the fact that you can extend \nits functionality with tons and tons of plugins.   And so many of these amazing plugins exist, \nit takes half a lifetime just to try a small   part of them. So much so I am considering making \nanother video just about that relatively soon. I think this is the best free and \nopen source project ever. And you   don’t have to start from zero, I always \nsay that you Fellow Scholars should try   these amazing demo files that also come for \nfree. And my friend Andrew Price’s legendary   donut tutorials are also available. \nBoth are in the video description. I have so many great memories writing my research \nworks within Blender, and I really hope that it   will bring many more to you Fellow Scholars. \nOnce again, all free for all of us. So good!   A big thank you to all the developers and everyone \nwho donated to make this happen. If you wish to   donate to them, a link is available in the video \ndescription. And now, let the experiments begin!",
      "research_papers": [],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [],
      "uploaded_date": "20250430",
      "duration": "5:34",
      "view_count": 77541
    },
    {
      "video_id": "hUVfAVjsfL4",
      "title": "NVIDIA’s New AI: Impossible Ray Tracing!",
      "description": "❤️ Check out DeepInfra and run DeepSeek or many other AI projects: https://deepinfra.com/papers\n\n📝 The #nvidia paper \"3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting\" is available here:\nhttps://research.nvidia.com/labs/toronto-ai/3DGUT/\n\n📝 Our Separable Subsurface Scattering paper: https://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\n📝 SSS For Gaussian Splatting: https://sss.jdihlmann.com/\n\nSources:\nhttps://x.com/jonstephens85/status/1911923445660893321?s=46\nhttps://x.com/jonstephens85/status/1908730004973986175?s=46\nhttps://www.youtube.com/watch?v=KPFPuXm2u7I\nhttps://www.youtube.com/shorts/si1-zFZpeDE\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu",
      "transcript": "There are two ways of creating \nand rendering a virtual world. This is what we call rasterization. This \nis what most computer games have used for   many years now. It is super fast and most of the \ntime, it looks good enough. But only if you feel   that you can live a happy life without super high \nquality reflections and refractions. But not me. Because for that, you need something more. My \nfavorite. Ray tracing. Look at these beauties.   This you can not get through rasterization, only \nthrough ray tracing, but there is a problem. It   simulates the path of millions and millions of \nlight rays, which takes super long to compute.   Like orders of magnitude longer, sometimes \nfrom minutes, to even weeks. Ouch. This one   is a beauty, reflections, refractions, volumetric \ncaustics, oh my. But it took us 3 weeks to render. So, rasterization, fast, limited, ray \ntracing, a complete solution, but slow. So I guess, choose one right? Well, \nscientists at NVIDIA has an absolutely   insane idea: they said, why not do both?\nWell, my opinion was before reading this   paper — don’t do both because it is impossible \nto pull off. These two ideas are like oil and   water — they don’t mix. You most likely end \nup with the disadvantages of both. That is,   a limited, but super expensive \nsolution. No thank you! So,   did they pull off the impossible? \nWe’ll get to that in a moment. And then, splat happens. I mean,   a paper called Gaussian Splats came out \nand took the world by storm. What is that? It’s a technique that thinks in terms of little \nGaussians, representing a scene like this as a   collection of little bumps if you will. And note \nthat these scenes can come from real life. You   just walk around with your camera, and then, you \ncan play in a digital version of it. And the cool   part is that it gets better. It is incredibly \nfast, easily faster than real time. So far so   good. However, here is the problem: mirror-like \nspecular reflections are not great. And many of   the beautiful ray tracing effects that you just \nsaw earlier don’t look so good there. It can also   eat up quite a bit of memory, which is scarce \nand expensive on graphics hardware these days. But the limitations don’t stop there: it doesn’t \nsupport any fancy camera models. For instance,   fisheye cameras? Don’t even think about it. Also,   no camera tricks like rolling shutter. So \nthere is tons of potential in Gaussian splats,   but if I think about the fact that we get \nno refractions, none of the good stuff that   we Fellow Scholars love so much. This all \nranges from very difficult to impossible. But don’t despair, because here comes \nthe impossible, but at the very least,   insane idea: let’s do both rasterization \nand ray tracing at the same time.  This research work does it by taking Gaussian \nSplatting, and adding to it something they   call secondary rays. This means that there \nare finally, rays of light in the system,   and they are allowed to bounce around. So, \nare you thinking what I am thinking? Oh yes,   hold on to your papers Fellow \nScholars and look at that! A proper virtual world running \nin real time, and high-quality   reflections. I am absolutely loving this \none. And, refractive objects like glass,   also…wow. Now let’s whip out that \nfisheye camera, and…yes! Finally,   this all used to be impossible, and now it \nis not only here, but it runs in real time. Here comes my favorite part: the \nname of this work. You’ll love   this. They call it the 3D Gaussian \nUnscented Transform. 3D G U T. Yup,   but that’s not how we will call it. No \nsir! We shall call it 3DGUT. Much better! And here is a nice little surprise. Do you know \nwhat super important AI area likes weird camera   models and rolling shutter? Training self-driving \ncars. Oh yes. And this is going to be super useful   for that too. But is it really better than \nprevious techniques? Let’s see together. Dear Fellow Scholars, this is Two Minute \nPapers with Dr. Károly Zsolnai-Fehér. So, 3DGUT looks amazing, but we are Fellow \nScholars here, we want to see the comparisons   against previous techniques. Well, turns out, \nthere is one earlier method that can extend   Gaussian Splats to fisheye cameras, but…oof. I \nsee lots and lots of artifacts and distortions   here. So do you have the guts to have a \nlook at the new technique? Let’s see…oh   my, that is a huge difference. The \nprevious one had trouble especially   with objects that are closer to the \ncamera, but this new one. Loving it. But the story doesn’t end \nhere. There is so much more   below the surface. We are going \nto look at this too in a moment. And here comes the best part: they didn’t \njust close it down to sell it to us. Nope.   All this is available right now for all of \nus for free. The source code is out there,   I’ve put a link in the video description, so \nI hope you Fellow Scholars have the guts to   play with it. Sorry, I promised myself not \ndo it again and I just did. But in fact,   some of you already do play with it! This Fellow \nScholar says that he trained it to only 30%,   and it already looks amazing, even through \nthe compression artifacts from social media. Okay, but I hear you asking, wait a \nminute Károly. What is this beauty?   Well, I said that there is more below \nthe surface. I said that because this   other work is about light transport that \nhappens within these objects. That is,   subsurface scattering. The art of \nrendering beautiful translucent objects. Now, you can do this in rasterization, here is \na work called Separable Subsurface Scattering,   it gives you super fast human skin, marble, \nmilk, you name it. I love working on things   like this so much. And it is so simple, when \nwe published it, we even implemented a version   of it that fits into 4 kilobytes. That is \ncrazy — it fits into a file that is smaller   than half a second of mp3 music. Much smaller. \nIt is available on my website, the link is in   the description. To the best of my knowledge, \nyou can also use it in Unreal Engine as well. So this is subsurface scattering, but for \nrasterization, not for Gaussian Splats. What   about that? Well, this amazing new work offers \nyou exactly that. And it gets even better,   it supports relighting too. So you \nhave your object, you add an image,   and it imagines what that object would look \nlike in these different environments. Super fun. And if you don’t like what you got, \nyou can even edit these materials,   so you can go from skin to glass to wax. \nI mean, just think about that. Gaussian   Splats are not old at all, it’s about 2 \nyears old. And now, through the power of   open research and human ingenuity, and \na hint of AI, reflections, refractions,   and translucency is already possible for Gaussian \nSplats. Yes, now you can get reasonably high   quality translucent objects in these virtual \nworlds too. So, next level computer games and   virtual worlds are coming! And self-driving cars \ncan also learn better. I am kind of stunned that   almost nobody is talking about these papers. \nBut this is why Two Minute Papers exists. And don’t forget, we are all Fellow Scholars \nhere, so we love to look at these papers for   more information, and it goes without saying that \neverything is available in the video description. So, what do you think? What \nwould you Fellow Scholars   use this for? Let me know in the comments below.",
      "research_papers": [
        {
          "type": "mentioned",
          "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
          "context": "mentioned in video",
          "source": "video_content"
        }
      ],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [
        "paper_analysis_agent"
      ],
      "uploaded_date": "20250428",
      "duration": "8:50",
      "view_count": 250474
    },
    {
      "video_id": "pC8wRC-1PmQ",
      "title": "OpenAI’s ChatGPT o3 - Pushing Humanity Forward!",
      "description": "❤️ Check out Vast.ai and run DeepSeek or any AI project: https://vast.ai/papers \n\n📝  OpenAI's o3 is available here:\nhttps://openai.com/index/introducing-o3-and-o4-mini/\n\n📝 The paper \"Humanity's Last Exam\" is available here:\nhttps://agi.safe.ai/\n\nChatGPT trick:\nGo to your profile - personalization - customize ChatGPT, and add:\n“Look for peer-reviewed sources for every answer. Rate the credibility of that source on a scale of 0 to 10 for every source. And of course - use more emoji. ”\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nSources:\nhttps://x.com/DeryaTR_/status/1914133246465487026/photo/1\nhttps://x.com/nicdunz/status/1913043509348643323\nhttps://x.com/mckbrando/status/1913268371266932865\nhttps://x.com/emollick/status/1913471315807191310\nhttps://www.reddit.com/r/singularity/comments/1k1819k/o3_can_solve_wheres_waldo_puzzles/\nhttps://www.reddit.com/r/singularity/comments/1k1819k/comment/mnk0sbn/\nhttps://www.mdpi.com/2076-3417/12/10/5255\n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu\n\n#openai",
      "transcript": "",
      "research_papers": [
        {
          "type": "mentioned",
          "title": "Humanity's Last Exam",
          "context": "mentioned in video",
          "source": "video_content"
        }
      ],
      "key_concepts": [
        "gpt",
        "openai"
      ],
      "implementation_ideas": [
        {
          "type": "model_implementation",
          "title": "Transformer Model Implementation",
          "description": "Build a custom transformer model for specific use case",
          "complexity": "high",
          "estimated_time": "3-4 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent",
        "paper_analysis_agent"
      ],
      "uploaded_date": "20250424",
      "duration": "9:22",
      "view_count": 83199
    },
    {
      "video_id": "wq8BgIfOxnk",
      "title": "NVIDIA’s Tech: Brutal 2,500,000 Part Simulation!",
      "description": "❤️ Check out Vast.ai and run DeepSeek or any AI project: https://vast.ai/papers \n\n📝 The papers are available here:\nhttps://www.dgp.toronto.edu/projects/trading-spaces/\nhttps://pcs-sim.github.io/pd/\nhttps://visualcomputing.ist.ac.at/publications/2024/SDTF/\nhttps://starryuniv.cn/files/sig24magnetic.pdf\nhttps://github.com/Univstar/IoB-Ferrofluid-2D\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu\n\n#nvidia",
      "transcript": "This is an amazing paper that is going to \nabsolutely brutal. Yikes… I kinda love it. So   what is going on here? You see, most simulations \nin computer games are about simulating solids.   That’s the boring stuff. But what about when \nwe try to destroy things and they deform? And then, we will do increasingly crazier \nthings, like this, and then this, and at the end,   this insane thing. And then I’ll tell you \nwhy I am heartbroken. Dear Fellow Scholars,   this is Two Minute Papers \nwith Dr. Károly Zsolnai-Fehér. Hmm…yes! Now I see the appeal of simulating \ndeformations, that would be amazing,   but doing this on a larger scale, that is so \nmuch more difficult, and takes much, much longer.   Dropping a spiky mace on this city is a beauty, \nbut some of these simulations can take 3 hours   to compute or even longer, wow, that is brutal. \nI mean, the simulation, and the waiting time too. But wait, maybe this new paper \ncan help us. Although I doubt it,   because it would take simulating \n2.5 million tetrahedra, these   tiny little elements. That sounds \nvery painful. Like, this painful. However, it can compute a simulation, once \nagain, kinda brutal, and surprisingly,   it gets even better. Don’t forget, \nthis is a virtual world. Our world,   so what does that mean? It means we do whatever \nwe want. We can control the stiffness of these   objects with one physical parameter. Just change \nit, and then, look, we just made that jelly a bit   more rubberized. Fantastic. Do it some more, and \nnow the anvil barely bounces off of it. Loving it. So, let’s pop the question. Big breath - \nso, how fast is it? Now hold on to your   papers Fellow Scholars, because when I saw \nthis, my goodness — I couldn’t believe my   eyes. It is between 3 and 300 times faster \nthan previous methods. Some of the smaller   simulations take only a few seconds, so \nperhaps just one more paper down the line,   and we might just have this in real \ntime in our video games. Loving it. Now, when talking about cloth simulations, \njust look at that. This other work really   knows how to do these really tough \ntwisty cases. But it gets better. And   not just because it can simulate the flag \nof Catifornia flawlessly, well done there. But typically, when we want to achieve \nsomething in a computer game or animated movie,   we can’t compute a full-scale simulation \nlike this because it can take from hours   to days to compute. No-no. First, we \ncompute a coarse simulation quickly,   see if it has promise, but…oof. We have a huge \nproblem here. We can’t do this. Do you see why?   Making a finer version of a coarse \nsimulation behaves entirely differently. So we have to wait for days for the final \nsimulation. And this is not what I want…I mean,   having to wait so long to get a chance to \nthrow it a bit differently again. No thanks! But, with incredible method, look. Yes, that is \nwhat I want! A perfectly designed experiment.   We do the coarse experiment quickly, and \nclear all three rings. Finally! And normally,   we saw that we do the finer \nversion of the simulation,   something entirely different happens. And in \nthis case…I can’t believe it. Fast previewing   of a difficult simulation is now possible, and \nthe outcome remains the same when running the   full workload afterwards. I’ve never \nseen anything like this before. Bravo! So if you want to make cats kiss or have \nthese octocats fall into their own containers,   you can simulate that very, very quickly, \nand then only do the fine simulation once,   afterwards. Yes, these are us Fellow Scholars \nwhen seeing this amazing paper at work. And while computer games still have lots of \nproblems like this that we call z-fighting,   where often two seemingly simple objects just \ncan’t decide who should be in front. But,   look at this. Holy mother of Papers! In the \nmeantime, scientists are doing their best,   and, goodness, modeling crazy topology \nchanges with these beautiful bubbles is   also now possible. Just look at that. Imagine \nsitting down and having to write a handcrafted   computer program to be able to do all that. \nThis ingenuity is humanity at its best. Or with this, you can also stack a bunch \nof objects together in twisty ways,   and when you look inside. Let’s see…I \ndon’t see any fighting at all. Fantastic. And I kept the my favorite for last. \nOh my…are you seeing what I am seeing?   Simulating a piece of fluid \nthat is magnetic. Ferrofluids! It’s very simple, except the fact \nthat it is close to impossible.   Let me explain. All you need to do is to \nput a magnet under a piece of ferrofluid,   and these magical spikes start appearing. That \nis simple. Now sit down, and write a computer   program that is capable of simulating that. Now \nthat Fellow Scholars, is nearly impossible. Yes,   you have to understand and program all this \ncrazy stuff to be able to pull this off. So what is going on here? Well, this work offers \nsomething they call an Induce-on-Boundary solver.   What it can do is that it does not perform the \ncomputations on the entire 3D volume of the fluid,   only on the 2 dimensional surface of the \nfluid. Only compute on the shell. That   is much quicker. And they pulled it off in a \nway that offers more favorable computational   speeds than previous works, and can be \ndropped into an existing fluid simulator. And this is how you can create these amazing \nfluid mazes and other insane experiments. I   love these works so much. That is a problem \nI will tell you about it in a moment. Yes,   you still have to wait for quite a while, but you \nknow, for this kind of quality, I’ll let it slip. And you know, everyone talks AI this, AI \nthat, but I see AI as a tool to enhance   the minds of these incredibly brilliant \nresearchers. Just imagine what we will be   capable of just two more papers down \nthe line. What a time to be alive! And now, look at this. My heart is \nbroken as almost nobody is seeing or   talking about these amazing papers. Can you \nbelieve that? Here, on Two Minute Papers,   you can learn about works often no one else is \ntalking about, but here is the problem — it is   almost impossible to keep the flame alive \nfor simulation papers like this. You see,   a few hundred episodes ago, we had ones \nthat did really well, and Youtube kept   recommending these episodes. And I get it, I \nmean, look at this insane quality work. Wow. But unfortunately, Youtube is not recommending \nthem to you too much anymore, so every time I am   just here talking to myself whenever I do that. \nProbably this time too. Hello Károly! Hello,   how are you doing? Doing great, thank you! \nExcept that it’s been almost a thousand Two   Minute Papers videos, God is my witness I tried \neverything since. If it’s a simulation paper,   nothing works. I am heartbroken. I don’t really \nhave a solution, but if you keep watching these,   posting them, and recommending them to your \nfriends, maybe one day. Maybe. So thank you   for being with me on this journey for almost \n10 years now! This is my dream job and we   couldn’t exist without you Fellow Scholars. \nLet me know in the comments what you think.",
      "research_papers": [],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [],
      "uploaded_date": "20250420",
      "duration": "8:42",
      "view_count": 79028
    }
  ],
  "research_themes": [
    "Retrieval Systems"
  ],
  "top_concepts": [
    "rag",
    "neural network",
    "gpt",
    "openai",
    "anthropic",
    "reinforcement learning"
  ],
  "project_recommendations": [
    {
      "title": "Multi-Agent Research System",
      "description": "Build a system that automatically researches and implements AI concepts from video content",
      "priority": "high",
      "technologies": [
        "LangChain",
        "Vector Databases",
        "LLMs"
      ]
    },
    {
      "title": "Knowledge Graph Builder",
      "description": "Create a knowledge graph from research papers and video content",
      "priority": "medium",
      "technologies": [
        "Neo4j",
        "NLP",
        "Graph Algorithms"
      ]
    },
    {
      "title": "Automated Literature Review",
      "description": "System that automatically reviews and summarizes research papers",
      "priority": "high",
      "technologies": [
        "ArXiv API",
        "Transformers",
        "Summarization"
      ]
    }
  ],
  "analysis_timestamp": "2025-05-28T19:04:55.163046"
}