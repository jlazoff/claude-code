{
  "channel_name": "@TwoMinutePapers",
  "channel_url": "https://www.youtube.com/@TwoMinutePapers",
  "total_videos": 100,
  "videos": [
    {
      "video_id": "ZE0JZYgiaGc",
      "title": "NVIDIAâ€™s New AI: Impossible Weather Graphics!",
      "description": "â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\nğŸ“ The papers are available here:\nhttps://research.nvidia.com/labs/toronto-ai/WeatherWeaver/\nhttps://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/\n\nSource: https://www.youtube.com/watch?v=CVdtLieI5D0\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "What you are seeing here is basically impossibleÂ \nto do. Yet it just happened. So how? And what isÂ Â  going on here? Well, this is a brand newÂ \nAI technique where in goes your video,Â Â  and it does changes the weather effects on it. No 3D modeling required, no physics simulationÂ \nrequired, no camera calibration required. Crazy. And then, you shall hear about theÂ \nlegend of the apple that went intoÂ Â  another dimension. And how toÂ \nsolve this problem. And more. And I said impossible becauseâ€¦ look.Â \nThis is what happens when you tryÂ Â  previous techniques to do that. InÂ \ngoes this video, then, little AI,Â Â  add some fog to it. Oh my. I canâ€™t see anything!Â \nAnd it gets worse, it is not even realistic.Â Â  This AnyV2V technique was published lessÂ \nthan a year ago. Not some ancient technique. Okay, now, a different workâ€¦ouch. They clearlyÂ \ndonâ€™t understand how to do this properly. ItÂ Â  seems absolutely impossible. Until this newÂ \nwork. Look at that. Wow, this is crazy good!Â Â  And not only at fog. Rain, not a problem. Look atÂ \nthat beauty. It makes any place look like London.Â Â  Kidding. Snow synthesis is also incrediblyÂ \namazing, look at how cozy this is. Loving it. Okay-okay, but wait a minute. You can take anyÂ \ntechnique, choose that one scene it does reallyÂ Â  well on, and advertise only that. What we, FellowÂ \nScholars want to know is whether it is actually aÂ Â  practical technique. Does it work on a variety ofÂ \nscenes? You bet your papers it does. Look at that! A huge variety of scenes, and all of themÂ \nare absolute beauties now. Just think aboutÂ Â  training self-driving cars â€“ they could experienceÂ \ncountless weather scenarios safely in simulationÂ Â  thanks to this! So this was weather synthesis.Â \nNow get this, what about weather de-synthesis? Yes, what about removing weather effects? Now thatÂ Â  is an entirely different beast. IÂ \nthink that is truly impossible. Why? Partly because it is kind of like asking AIÂ \nto 'undo' milk from coffee. How is that evenÂ Â  possible? You see, if you wish to remove the fogÂ \nfrom this footage, none of the previous techniquesÂ Â  are really capable of doing that. No wonder, IÂ \nmean, adding rain or snow to already existingÂ Â  footage, okay, you need to change some pixelsÂ \naround. Tough, but possible. But when you removeÂ Â  the fog, new things appear. Oh my. You would haveÂ \nto synthesize and fill in new information. WhatÂ Â  could be in the background? Well, you have to putÂ \nsomething there that makes sense, and for that,Â Â  you need to not just draw a few pixels, no-no.Â \nFor that, you need to understand the world. So, can the new technique it do it? NowÂ \nhold on to your papers Fellow ScholarsÂ Â  andâ€¦oh my goodness. It did it. And it notÂ \nonly did it, but when you try rain removal,Â Â  previous techniques are either not reallyÂ \ndoing anything, or the only one that does,Â Â  in return, also changes the whole scene.Â \nWe did not have colorful cars in theÂ Â  input footage at all! And the new one, onceÂ \nagain, incredibly amazing. Same with snow. But now, check this out, because it getsÂ \neven better. Dear Fellow Scholars, this isÂ Â  Two Minute Papers with Dr. KÃ¡roly Zsolnai-FehÃ©r.\nYes, no one said that fog should be turned offÂ Â  like a light switch, sometimes youÂ \nwant a little of it. Not a problem,Â Â  you get a little slider that you can play with. InÂ \nthe case of fog, you can change the fog density,Â Â  for snow, oh I love this one, in the case ofÂ \nsnow, you can play with the amount of coverageÂ Â  that you wish to see there. And, a bonus,Â \npuddle coverage. What? Puddles? Now letâ€™sÂ Â  stop for a second there. Donâ€™t forget, puddlesÂ \nmake the roads more specular, which means thatÂ Â  they are reflective. But to become reflective,Â \nthey have to reflect something. And once again,Â Â  this is something that cannot just come from thinÂ \nair, this is something the AI needs to synthesize. You see, I am a light transport researcherÂ \nby trade, and I tried to resist the urgeÂ Â  to mention specular reflections,Â \nand of course, I failed. Again. So this is how this amazing weatherÂ \ntransition video was made in the intro. And here is the craziest thing about thisÂ \nwork: it kind of teaches itself. Originally,Â Â  it performed weather removal. They used it toÂ \ncreate pairs of the same scenes, one with rain,Â Â  and one without. Then, they used this dataÂ \nto train a weather synthesis AI model. So,Â Â  in a way, it kind of teaches itself.Â \nI think self-supervised bootstrappingÂ Â  would be a good term to call thisÂ \nbehavior. Absolutely mind blowing. Now, just imagine combining the weatherÂ \nsynthesis with this amazing technique. Yes,Â Â  this is the legend of the apple that disappearedÂ \ninto another dimension. What is going on here?Â Â  The problem here is that we have an inputÂ \nimage, and what would we like to extractÂ Â  from it? Everything. Absolutely everything.Â \nGeometry information, depth information,Â Â  material properties, everything. And then, useÂ \nthat to re-render this image in a differentÂ Â  way. Perhaps with different lighting.Â \nLittle alternative realities. So cool! However, not so fast. When a previous techniqueÂ \nlooks at this apple, and we are asking it toÂ Â  reconstruct the scene and rotate it. Oofâ€¦I amÂ \nseeing bad news already. Look. It underestimatedÂ Â  the shadow in the scene. And when we rotate it,Â \ngoodness. Thatâ€™s like a sandwich in a shady shop.Â Â  From the outside, it looks amazing, when you lookÂ \ninto it, you get a black hole. Nothing. Now letâ€™sÂ Â  see the new techniqueâ€¦oh my goodness. ItÂ \npulled it off. Proper inverse renderingÂ Â  right there. And once again, not just on oneÂ \nscene, but on more lifelike environments too. But the point is that if thisÂ \nis a proper AI inverse renderer,Â Â  then it can do so much more. For instance,Â \nyou can not only relight the scenes,Â Â  or look at them from a new viewpoint, no-no,Â \nget this, you can also edit the materials. AndÂ Â  the results are still photorealistic.Â \nLike editing reality itself. Crazy. And you can do this too. Wait aÂ \nsecondâ€¦I hear you asking, KÃ¡roly,Â Â  what happened here? Well, not all of theÂ \nobjects in this scene are real. This wasÂ Â  inserted by this new technique. Same here. YouÂ \ncan maybe kind of tell if you look closely,Â Â  but one more paper down the line fromÂ \nhere. Not a chance. Another impossibleÂ Â  problem solved with a new research paper.Â \nAnd while we look at these beautiful results,Â Â  this is Two Minute Papers, elsewhere, youÂ \nget McDonald's, here you get the papers,Â Â  proper research papers from a scientist. HopeÂ \nyouâ€™re enjoying it as much as I am enjoying it.",
      "research_papers": [],
      "key_concepts": [
        "rag"
      ],
      "implementation_ideas": [
        {
          "type": "rag_system",
          "title": "RAG System Implementation",
          "description": "Build a retrieval-augmented generation system",
          "complexity": "medium",
          "estimated_time": "2-3 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent"
      ],
      "uploaded_date": "20250526",
      "duration": "8:16",
      "view_count": 54293
    },
    {
      "video_id": "Pqx-gSiogjM",
      "title": "DeepMindâ€™s Veo3 AI - The New King Is Here!",
      "description": "â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\nğŸ“ More on Veo3 available here:\nhttps://deepmind.google/models/veo/\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "This was the state of the art inÂ \nAI video generation 2 years ago,Â Â  and now check this out, look atÂ \nwhat just arrived, but also, listen. Oh my goodness. We have a new king. Yes,Â \nGoogle DeepMind just announced their newÂ Â  AI video generation technique, Veo3 where youÂ \nwrite a small piece of text, and out comesÂ Â  video. And it synthesized the sounds as well.Â \nNot only that, but for speech too. Wow! Thatâ€™sÂ Â  one of the hardest things to get right, becauseÂ \nas humans, weâ€™re wired to closely watch faces andÂ Â  detect even the slightest emotional cues whileÂ \nsomeone is speaking. In short, they have reallyÂ Â  been cooking. This is incredible, but it gets evenÂ \nbetter because it can do 10 amazing more things. One, scene changes. Did you notice that inÂ \nmost of the AI video generators out there,Â Â  you write a little text prompt, and you getÂ \none scene. But no meaningful changes. Well,Â Â  not here, these scenes really tell stories,Â \nlike a feather getting stuck in a spiderweb,Â Â  us meeting the mosquitoes of the future inÂ \na futuristic hiveâ€¦kidding. Or a paper boatÂ Â  getting lost. All of these come outÂ \nfrom one prompt each. Game changer! Two, reference-powered videos.Â \nYou take a photo of a character,Â Â  perhaps yourself, and a scene, and bam!Â \nYou are immediately there doing what youÂ Â  specified in the text prompt. With this,Â \nyou can appear in glorious places youâ€™veÂ Â  never been to. Perhaps places thatÂ \ndonâ€™t even exist. Loving this one. Three, style matching. To create a videoÂ \nof this amazingly creative origami world,Â Â  you donâ€™t need to do much, just fold somethingÂ \nas a style reference, give it as an image,Â Â  write a piece of text, and you canÂ \ncreate a whole feature-length movie. Four, finally, character consistency forÂ \nvideo! This is barely solved well enoughÂ Â  for still images, and they did it for completeÂ \nvideos. And you really get the same character,Â Â  or can even create interesting variants ofÂ \nit. My head is still spinning from this. But this is nothing compared to whatÂ \nis coming now. Dear Fellow Scholars,Â Â  this is Two Minute PapersÂ \nwith Dr. KÃ¡roly Zsolnai-FehÃ©r. Five, specify the first and the last frame.Â \nStart: a block of marble or stone. Last frame:Â Â  this griffin. Now, little AI, you do theÂ \nhard part â€” everything in between. So,Â Â  can it do it? No way, right?Â \nAndâ€¦wow! This is breathtaking. Six, if you get a scene where youÂ \nwish to zoom in, that is easy,Â Â  you just do it yourself. But zooming out,Â \nthatâ€™s a nearly impossible problem, becauseÂ Â  you would have to synthesize all the missingÂ \ninformation. And all this for video. Can itÂ Â  do that too? Look at that. It seems perfect to meÂ \nâ€” I canâ€™t even point out a single seam anywhere. Seven, add an object to an already existingÂ \nscene. Or even a human. And they really knowÂ Â  how to make KÃ¡roly happy. Look. IndirectÂ \nillumination is there. That is the colorsÂ Â  of the burning torch painting itsÂ \nsurroundings. Absolutely beautiful. Eight, character control. You record a videoÂ \nof yourself, add a target image of the subject,Â Â  and there you go. Now I am a littleÂ \nmore excited about this variant. Oh yes,Â Â  making virtual characters comeÂ \nalive. What a time to be alive! Nine, you can also mark up an image with movementÂ \ndirections. And nowâ€¦ try to imagine what a goodÂ Â  result would look like. Now, Veo3â€¦I am outÂ \nof words. This is low-key amazing. Why? Well,Â Â  it does it exactly as we specified, but the blocksÂ \ndonâ€™t collide, and the whole scene just makesÂ Â  sense. You could never create a computer programÂ \nwithout AI that can pull off anything like this. Now, ten is coming in a moment. In theÂ \nmeantime, Iâ€™ll note that not even thisÂ Â  technique is perfect, the sounds ofÂ \nthis keyboard are still a bit off. Donâ€™t forget, not all, but most of these videosÂ \nhave sound too that you can check out throughÂ Â  the link in the video description. And 10, theÂ \nScholars who know. You see, if you watched TwoÂ Â  Minute Papers all this time, you knew thisÂ \nwas coming. 6 years and 600 episodes ago,Â Â  we talked about a paper about an AI system thatÂ \nwas able to guess the sound of pixels. Then,Â Â  10 months ago Google DeepMind published a followÂ \nup work on it as well that we talked about. So,Â Â  all you need to do is watch Two Minute Papers, andÂ \nyou will know the future. Bravo Google DeepMind,Â Â  and now, step number two, we might get fullyÂ \nopen solutions that will be able to do similarÂ Â  things. Imagine running this at home, for free. IÂ \nlove being alive today. What a time to be alive!",
      "research_papers": [],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [],
      "uploaded_date": "20250522",
      "duration": "6:15",
      "view_count": 73518
    },
    {
      "video_id": "T0eWBlFhFzc",
      "title": "DeepMindâ€™s AlphaEvolve AI: History In The Making!",
      "description": "â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\nğŸ“ AlphaEvolve: https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\nğŸ“ My genetic algorithm for the Mona Lisa: https://users.cg.tuwien.ac.at/zsolnai/gfx/mona_lisa_parallel_genetic_algorithm/\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "Today you might be witnessing history in theÂ \nmaking. I am so excited to tell you about this,Â Â  I cannot wait any longer. This newÂ \npaper called AlphaEvolve and it isÂ Â  absolute insanity. I had the honor ofÂ \nhaving a look at it earlier before itsÂ Â  release. And itâ€™s been days nowÂ \nand my head is still spinning. You see, about 900 of our videos ago or 9Â \nyears ago, this was the state of the art in AI.Â Â  Yes. Now you might think I flipped outÂ \nbecause a neural network learned to rotateÂ Â  this car into a frontal position, or thatÂ \nit sorted these lines. But no. Not at all.Â Â  I flipped out because it learned not just toÂ \nsolve a task at hand, but because it learnedÂ Â  create an algorithm to do that. I thoughtÂ \nthis concept might make it big one day. And now, 9 years later, finally, this is the day.Â \nHere is AlphaEvolve. An evolutionary coding agentÂ Â  that grows algorithms and computer codeÂ \nout of nothing. It is absolutely crazy. It was given 50 open problems in mathematics,Â \ngeometry, combinatorics and more. And in 75% ofÂ Â  the cases, it was able to rediscover the bestÂ \nsolution the smartest humans were able to comeÂ Â  up with so far. That is already insanity, butÂ \nthatâ€™s nothing, because in 20% of the cases,Â Â  it even improved upon previous solutions. Yes,Â \nan AI that is able to push humanity forwardÂ Â  in a way that no human can. I think this is aÂ \nhistoric moment, but get this â€” there is more. It also discovered a faster matrix multiplicationÂ \nalgorithm than one of the landmark techniques,Â Â  which is called the Strassen algorithm.Â \nThat is ridiculous. This area is so old,Â Â  so saturated with so many works, it is almostÂ \nimpossible to come up with something that isÂ Â  just a tiny bit, maybe 1% better. They sayÂ \nthat no one was able to do that in 56 years,Â Â  and after the 56 years, not a humanÂ \ndid it. But an AI technique did. Wow. This means that we are in for a crazy AI rideÂ \nwhere these methods will start to improveÂ Â  things that we thought are impossibleÂ \nto improve. For instance, donâ€™t forget,Â Â  AI techniques themselves are also running onÂ \nmatrix multiplication, so what does it mean? Well, now hold on to your papersÂ \nFellow Scholars and check this out:Â Â  it learned to improve circuit designs forÂ \nchips meant to run these AI techniques,Â Â  and it also improved its own training algorithm.Â \nYes, you heard it correctly. It is able to createÂ Â  a better piece of hardware that it runsÂ \non, and it also improves its own code. That sounds insane. So, how does it do it? Well, first, you can mark a piece of code that youÂ \nwant to evolve, some logic to decide whether a newÂ Â  algorithm is better or worse, and any commentsÂ \nthat you might have. And then, off it goes inÂ Â  an evolutionary loop. Then, a new piece of codeÂ \nemerges, that gets better and better over time. So â€” is that new? Nope. No-no-no. Evolutionary algorithms have existed for aÂ \nwhile now. For instance, you see my geneticÂ Â  algorithm building up the Mona Lisa fromÂ \na bunch of triangles. You start out withÂ Â  a random configuration, and it over time,Â \nevolves into the correct solution. It veryÂ Â  loosely mimics how evolution works in nature. TheÂ \nlink is in the description. I wrote this by hand,Â Â  and it shares some very rough, basic concepts withÂ \nAlphaEvolve. But AlphaEvolve is a supercharged,Â Â  AI-infused variant of that. It is spaceÂ \ntechnology compared to this program. By the way, AlphaEvolve also solved a variant ofÂ \nthe kissing problem too. What is that about? Well,Â Â  this is not about a robot doingâ€¦whateverÂ \nthe heck it is doing here. Nope. This isÂ Â  one of the crazy names mathematicians like toÂ \ncome up with. And if you think these spheresÂ Â  kissing each other is so weird, well then, lookÂ \nat this. Oh my goodness. This is the hairy ballÂ Â  theorem. It doesnâ€™t have anything to do withÂ \nthis new AI, but I couldnâ€™t resist mentioningÂ Â  it. Mathematicians and their weird namingÂ \nschemes. Youâ€™ll get used to it, donâ€™t worry. So, what happens now? Well, I think IÂ \nhave an idea, and itâ€™s pretty insane.Â Â  Dear Fellow Scholars, this is Two MinuteÂ \nPapers with Dr. KÃ¡roly Zsolnai-FehÃ©r. I think they are going to Alpha allÂ \nthe things. But I hear you asking,Â Â  KÃ¡roly, what do you mean by that? Well, earlier DeepMind came up with AlphaGo. AnÂ Â  AI-based technique that was ableÂ \nto play Go on a superhuman level. Then, AlphaStar, to play the strategy gameÂ \nStarcraft at the very least on a level of aÂ Â  human champion. Perhaps even better.Â \nBut both of these are the wrong wayÂ Â  to think about their Alpha project.Â \nThese are all experiments to createÂ Â  generally intelligent AI techniquesÂ \nthat can learn and do so much moreÂ Â  than just play Go and StarCraft. AndÂ \nAlphaEvolve is finally one of these. This one learned something so much more generalÂ \nâ€” computer coding. And it can code up so muchÂ Â  more than just Chess or Go. You see, computerÂ \ncode underpins almost everything we do. So,Â Â  yes, with that, it will be able to designÂ \nbetter hardware for itself to run on, andÂ Â  it gets better â€” perhaps even the next version ofÂ \nAlphaEvolve will be written by AlphaEvolve itself. Get it now? Sir Demis Hassabis, DeepMindâ€™s CEOÂ \nkeeps saying that step number one is solvingÂ Â  intelligence. And step number two is usingÂ \nthis intelligence to solve everything else. He says the following: â€œI think oneÂ \nday, maybe we can cure all diseaseÂ Â  with the help of AI. Maybe within theÂ \nnext decade. I don't see why not.â€ Once again: cure all disease perhapsÂ \nin the next decade. Sounds crazy,Â Â  right? But donâ€™t forget, this is not comingÂ \nfrom some guy, this is coming from a personÂ Â  who just won a Nobel Prize in chemistry withÂ \nhis AI technique. You know what? I could notÂ Â  imagine how this could be true, but withÂ \nAlphaEvolve, you know, now I am a believer. So, letâ€™s Alpha all the things, and withÂ \nthe help of human ingenuity plus AI,Â Â  we might be able to cure all disease inÂ \nthe next decade. What a time to be alive!",
      "research_papers": [],
      "key_concepts": [
        "neural network"
      ],
      "implementation_ideas": [],
      "agent_recommendations": [
        "research_agent"
      ],
      "uploaded_date": "20250517",
      "duration": "7:36",
      "view_count": 160459
    },
    {
      "video_id": "g32Candon3A",
      "title": "New AI: Impossible Creatures Come Alive!",
      "description": "â¤ï¸ Check out DeepInfra and run DeepSeek or many other AI projects: https://deepinfra.com/papers\n\nğŸ“ The papers are available here:\nhttps://anytop2025.github.io/Anytop-page/\nhttps://zhongleilz.github.io/Sketch2Anim/\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "Alright, two top-tier, absolutely amazing AIÂ \npapers today. This AI animates creatures itÂ Â  has never seen before. And hereAnd here,Â \neven a dinosaur can learn something fromÂ Â  a mystery creature. And this one makes aÂ \npoorly made drawing into a fully-fledgedÂ Â  3D animation. Both solve problemsÂ \nthat I thought were impossible. Oh my. So, what is this? People talk a great deal aboutÂ \nbreathing life into virtual characters. That isÂ Â  computer animation. But whenever they talk aboutÂ \ncomputer animation, they are talking about movingÂ Â  virtual humanoids around. Humans, mostly. ButÂ \nwhat about these guys? Can we still animate them? Okay, the problem is the following: youÂ \nare not an animator, so you give an AIÂ Â  the shape of the character that they callÂ \nskeleton, and out comes how it would move. And then of course, these skeletonsÂ \nare just an underlying structure,Â Â  and the final appearance of theÂ \ncharacter is of course, on top of that. Well, people tried to do thatÂ \nwith automated techniques, butÂ Â  the results wereâ€¦unsatisfactory to say the least.Â Â  But with this new methodâ€¦ surprise! These areÂ \nreally believable motions on a wide variety ofÂ Â  icky creatures. That would already be good, butÂ \nthis is nothing compared to what is coming now. These animals donâ€™t just move around, theyÂ \ncan learn from each other. For instance,Â Â  this dinosaur is doing somethingâ€¦interesting.Â \nStanding on one leg? Hmmâ€¦where did it learnÂ Â  this from? Oh yes, it learned it from aÂ \nflamingo. Imagine that! And note that theÂ Â  insane part is that this is not justÂ \ncopying, it has learned the motion,Â Â  and has adapted it to its own body. That isÂ \nabsolutely incredible. And with this system,Â Â  a chicken can also learn how to jumpÂ \nfrom an ostrich as well. Loving it. So, adaptation is possible. But how far canÂ \nwe push it? Well, letâ€™s give it a try. Here,Â Â  scientists have excluded the cat and the komodoÂ \ndragon from the training set, so the AI has neverÂ Â  seen them. And if we do that with a previousÂ \ntechnique, unfortunately it seems completelyÂ Â  clueless. Like a confused pile of digitalÂ \nlimbs having an existential crisis. No wonder,Â Â  because it hasnâ€™t seen this kind of animalÂ \nbefore. Now letâ€™s see what the new one can doâ€¦IÂ Â  canâ€™t believe it. Motions generatedÂ \nfor unseen animals. This is insanity! So how on earth does it do that? Well, it hasÂ \nlooked at a bunch of animals moving around,Â Â  and there are parts in different animals that areÂ \nsemantically similar. Parts that have a similarÂ Â  function. For instance, arms and legs are aÂ \ngeneral concept, and if you look at a new animal,Â Â  having seen a lot of different kinds of arms andÂ \nleg morphologies in action, you may be able toÂ Â  infer how it is used. You Fellow Scholars can doÂ \nthat, of course, because you are all brilliant.Â Â  Man, I love my audience. But the incredibleÂ \npart is that here, a machine is doing that.Â Â  Generalizing knowledge to unseen examples.Â \nAnd to me that sounds like intelligence. Wow. Youâ€™ll love this one. Look. So we can even peerÂ \ninto what it is thinking about these motions.Â Â  It understands when this animal is relaxed, andÂ \nwhen it is attacking. And to demonstrate that itÂ Â  really understands this, it shows similar motionsÂ \nin completely differently shaped animals. Yes,Â Â  it has learned the concept of attacking.Â \nSame vibe, different body. So cool! And it also understands when other, whenÂ \ndifferent kinds of motions like fallingÂ Â  and growling start and when they end.Â \nAnd how they look in different animals. But it gets even better. Now holdÂ \non to your papers Fellow Scholars,Â Â  because it can even take an input motion thatÂ \nis incomplete, and when it switches to orange,Â Â  you see how the AI synthesized the rest of theÂ \nmotion. Goodness, this is like image inpainting,Â Â  filling out missing parts of anÂ \nimage, only for animation. Brilliant. And nowâ€¦look. Are you kidding me? Here, they tookÂ \nan animation of some body parts and left out theÂ Â  rest, and the orange part, for instance, theÂ \nupper body motion, is then simulated by the AI. The source code is freely available for allÂ \nof us to experiment with. Thank you so much! Now, imagine combining it with a techniqueÂ \nthat could take a character, in this case,Â Â  humanoids. Then, you just draw a line to indicateÂ \nwhat it should be doing, and it would perform theÂ Â  motion accordingly. That is of course possible,Â \nand that technique is called, becoming an artistÂ Â  who is excellent at their job. But doing thisÂ \nautomatically with AI? Not a chance. Why? Dear Fellow Scholars, this is Two MinuteÂ \nPapers with Dr. KÃ¡roly Zsolnai-FehÃ©r. Iâ€™ll show you why. For instance, letâ€™sÂ \npunch something this way and see if thatÂ Â  is possible. Here is the line, now,Â \nlittle AI, you do the rest. Well,Â Â  it is loading up, but thenâ€¦hey! NothingÂ \nhappens. Where is the punch? This other one,Â Â  look carefully. Why did it do that? Here is theÂ \nproblem: we, humans, understand the world in 3D,Â Â  even if shown just a piece of drawing onÂ \na 2D paper, but this technique doesnâ€™t.Â Â  It does not understand that we meant the line isÂ \nforward, not sideways. But of course, once again,Â Â  how on earth would it understand it? If we wantedÂ \nto move backwards, we would use the same line. SoÂ Â  it would kind of have to understand theÂ \nintention behind the drawing as well,Â Â  which is of course, impossible. No AIÂ \ncan read minds, so even when we wouldÂ Â  try a new method to do thaâ€¦.wait a minute. ItÂ \nunderstood! This is exactly what we wanted! Wow. So, yes, with this other technique, we can createÂ \na simple storyboard without any artistic training,Â Â  and, it works incredibly well already. OneÂ \nmore problem that used to be impossible,Â Â  which is now, possible. It is truly stunningÂ \nwhat it is capable of, and once again. Yup.Â Â  Look at that. These papers might as well beÂ \nsome secret superprojects, because almostÂ Â  nobody knows about them. This is why I am makingÂ \nTwo Minute Papers, and I hope you appreciate itÂ Â  too. This is arcane knowledge of the impossible.Â \nI donâ€™t think you hear about these anywhere else.",
      "research_papers": [],
      "key_concepts": [
        "rag"
      ],
      "implementation_ideas": [
        {
          "type": "rag_system",
          "title": "RAG System Implementation",
          "description": "Build a retrieval-augmented generation system",
          "complexity": "medium",
          "estimated_time": "2-3 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent"
      ],
      "uploaded_date": "20250515",
      "duration": "7:33",
      "view_count": 39586
    },
    {
      "video_id": "7EA5JM1DI9Y",
      "title": "NVIDIAâ€™s New AI: Impossible Video Game Animations!",
      "description": "â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\nğŸ“ The #NVIDIA paper \"GENMO: A GENeralist Model for Human MOtion\" is available here:\nhttps://research.nvidia.com/labs/dair/genmo/\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nSources for SLAM:\nhttps://www.youtube.com/watch?v=2GJuEIh4xGo\nhttps://www.youtube.com/watch?v=CEC5UwPV9gY\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "This amazing fresh new AI work from NVIDIAÂ \njust came out, and it is unbelievable,Â Â  and can do the craziest things. My goodness, thisÂ \nis amazing. Okay, whatâ€™s going on here? Well,Â Â  we have seen earlier AI-based works about textÂ \nto motion. Writing a short prompt and then,Â Â  it makes a virtual character move accordingly.Â \nBut I bet you havenâ€™t seen this: what is this?Â Â  This is called GENMO and it is not text toÂ \nmotion, no-no â€” this is everything to motion. For instance, one, you can start outÂ \nfrom a recorded video of yourself,Â Â  and it learns and transfers those movesÂ \nto a virtual character. That is crazy,Â Â  because it has to look at 2D pixels, and convertÂ \nthem into moving around with joints and limbsÂ Â  in 3D. So we climbed the stairs, nice, now what?Â \nWell, two, now we want to do a lunge on one leg,Â Â  but we are a bit lazy. Do you know what? JustÂ \nadd a text prompt and ask the AI to do it. This is incredible. But it gets better! Now, three, add some music as an input.Â \nThat of course, I canâ€™t show you,Â Â  becauseâ€¦you know the copyright situationÂ \non Youtube. So is this good? Well,Â Â  this is how I walk to work every day becauseÂ \ndoing this is my dream job. And now, phase four,Â Â  now thatâ€™s what I call a more challenging case,Â \nand wow, I am very impressed by how well it isÂ Â  able to look at a video, no other data, and givesÂ \nyou the right solution. I am loving this already. Now, letâ€™s start with somethingÂ \nsimple, but funny. Little AI,Â Â  climb up the invisible stairs, realizeÂ \nthat you are standing on invisible stairs,Â Â  and then see how much weight it canÂ \nsupport. Excellent construction! And now, letâ€™s make it harder. See theseÂ \nsilhouettes? These are keyframes. It hasÂ Â  to take these positions at the exact time on theÂ \ntimeline we said. Also, different kinds of inputs.Â Â  Now I really want to see this. First phase, walkÂ \nand look around, nothing crazy there, and now,Â Â  oh my, nailed it. Next one, also fine. AndÂ \nthen, phase 3 with the video. This is fantastic. And now comes the interesting part. You see, itâ€™sÂ \ntough to be able to take all of these inputs,Â Â  but the real challenge is weaving them together atÂ \nthese breakpoints. And if you look carefully now,Â Â  you will truly understand why this paper isÂ \nabsolute magic. Look. We change the initialÂ Â  footage, then we transition into the lunges,Â \nandâ€¦oh my goodness. It takes the style of theÂ Â  previous motion into account and doesÂ \nthe lunges accordingly. Absolute magic. I can imagine just firing up a Lambda instance,Â Â  and getting all of these resultsÂ \nin just a few seconds. Wow. And now, this is where the magic happens. If youÂ \ncreated your scene, and you are not happy withÂ Â  the timings, you can just edit it. Look. SuperÂ \nintuitive, also super challenging. Not for you,Â Â  but for the AI. You see, with this, you canâ€™tÂ \njust cut off part of the animation as it getsÂ Â  shortened, no, it has to re-do it from scratchÂ \nand make it transition into the next motionÂ Â  seamlessly. So, it is seamless? Letâ€™s seeâ€¦I thinkÂ \nit is as smooth a transition as it gets. Bravo! So, letâ€™s pop the question. Can itÂ \nactually deal with real dancing,Â Â  movements of people who do this for a living? Letâ€™s start with the easier case. It can danceÂ \ncha-cha-cha, I can recognize it being cha-cha-cha,Â Â  so thatâ€™s good. But now, hold on to your papersÂ \nFellow Scholars, and look at this. Real dancersÂ Â  from real clips. That cannot possibly work,Â \nor so I thought at first. Look. Wow! TheseÂ Â  are the best results Iâ€™ve seen in this area. AndÂ \nI cannot stop looking at them. And donâ€™t forget,Â Â  this is not pose estimation. Not just marking upÂ \nthe video with a doll in 2D. These are actual 3DÂ Â  joint and limb movements that are produced byÂ \nthe AI. Stunning. But it gets funnier. How? Dear Fellow Scholars, this is Two MinuteÂ \nPapers with Dr. KÃ¡roly Zsolnai-FehÃ©r. Check this one out. It is nowÂ \nacting like a monkey. Hmmâ€¦thisÂ Â  person looks like they are typingÂ \nsomething on a giant keyboard.Â Â  Oh no! He is posting some nonsense aboutÂ \nAI on Twitter! Actually, thatâ€™s right,Â Â  I think this one is the most realistic of themÂ \nall. It is acting like a monkey isnâ€™t he? Bravo! And if you are not a dancer, donâ€™t worry,Â \njust sit in your chair with a coffee, andÂ Â  write as many prompts as you want, even in shortÂ \nsegments, and it doesnâ€™t seem to break a sweat.Â Â  An absolutely fantastic AI contributionÂ \nto computer games and virtual worlds,Â Â  it looks like the real deal, and we still gotÂ \nit earlier than GTA 6. What a time to be alive! Now, limitations: only handles full-body motion,Â Â  no facial gestures, no hand articulation,Â \nI think that is quite clear. It is not yetÂ Â  an end to end solution as it also reliesÂ \non an off-the-shelf SLAM method, that is,Â Â  simultaneous localization and mapping. ThisÂ \nhelps extracting useful information out ofÂ Â  the videos like the camera position and directionÂ \nand hand it off to this AI technique. In short:Â Â  it cannot do every processing needed alone, butÂ \nit is close. Now, this has a heavy diffusionÂ Â  backbone, it needs 5 denoising steps, so I thinkÂ \nit might not be real time yet, but nearly there. Once again, super fresh new paper,Â \nand almost nobody is talking aboutÂ Â  this. Thatâ€™s crazy town. Thatâ€™s why manyÂ \nof you come and watch Two Minute Papers,Â Â  and I thank you for the honor of coming withÂ \nus all this journey for such a long time now,Â Â  as this is paper and episode numberÂ \n962â€¦yes! Almost a thousand papers,Â Â  and you Fellow Scholars just keep showingÂ \nup. Huge honor, thank you so much! Now I did not see that the sourceÂ \ncode would be published for this work,Â Â  but knowing NVIDIA, this is the kind of workÂ \nthey usually give us the source code for,Â Â  so fingers crossed. And then, justÂ \nimagine what we will be capable ofÂ Â  just two more papers down the line.Â \nThat is the First Law of Papers.",
      "research_papers": [
        {
          "type": "mentioned",
          "title": "GENMO: A GENeralist Model for Human MOtion",
          "context": "mentioned in video",
          "source": "video_content"
        }
      ],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [
        "paper_analysis_agent"
      ],
      "uploaded_date": "20250511",
      "duration": "7:14",
      "view_count": 118045
    },
    {
      "video_id": "tj3l1BGkRJI",
      "title": "OpenAIâ€™s ChatGPT Surprised Even Its Creators!",
      "description": "â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambda.ai/papers\n\nGuide for using DeepSeek on Lambda:\nhttps://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/?utm_source=two-minute-papers&utm_campaign=relevant-videos&utm_medium=video\n\nOpenAI post: https://openai.com/index/expanding-on-sycophancy/\n\nPaper on agreeableness: https://arxiv.org/abs/2212.09251\n\nSource: https://x.com/georgejrjrjr/status/1917722125668081863/\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "ChatGPT is an amazing tool that helps us withÂ \nour daily lives, purchases, it now helps medicalÂ Â  professionals make decisions, it can writeÂ \ncode, and even helps scientists move humanityÂ Â  forward. That is excellent. However, somethingÂ \nreally unexpected just happened. You see,Â Â  there are two key steps to training such an AIÂ \nchatbot: first, eating lots and lots of trainingÂ Â  data to build up knowledge. To understand theÂ \nworld around us. Now comes step number two:Â Â  then, we have to teach it how to behave. How toÂ \nbe a good assistant. See these thumbs up and downÂ Â  icons? With this, you can give feedback on whetherÂ \nthe assistant did well or not. Didnâ€™t like that itÂ Â  was too verbose? Just press thumbs down. Or did itÂ \nnail the solution to your math problem? Thumbs up. This is data that might be used to create theÂ \nnext version of the assistant. It is calledÂ Â  reinforcement learning with human feedback,Â \nRLHF. Now this part is much newer than justÂ Â  training a neural network to know things,Â \nand here, unexpected things can happen. And boy, did unexpected things happen. HereÂ \nare three of them. One, an earlier version ofÂ Â  ChatGPT unexpectedly stopped talking in Croatian.Â \nWhat? How could such a thing happen? At first,Â Â  scientists did not know why, but then they foundÂ \nthat Croatian people were much more likely to useÂ Â  the thumbs down button than people at other placesÂ \naround the world. And thus, the AI decided well,Â Â  if I am not doing well here, I am outta here. AndÂ \nby that, it meant stopping to speak Croatian. Wow.Â Â  Croatians got ghosted by an AI harder thanÂ \nany dating app could possibly dream of. So,Â Â  yes, user feedback can be culturally biased.Â \nSo how do you build an unbiased system withÂ Â  data that is biased? How do you take that intoÂ \nconsideration? Some people around the worldÂ Â  have different thresholds for good and bad,Â \nand some may not use the feedback buttons atÂ Â  all. How do you build a system that is resilientÂ \nagainst all of these? That is a tough problem. Two, in a more recent case, the new o3Â \nassistant suddenly started writing wordsÂ Â  in British for no clear reason.Â \nPerhaps next time it will demandÂ Â  tea breaks as well. I would loveÂ \nto know the reasoning for that,Â Â  but there is a third one, which is perhaps theÂ \nmost important, and most insidious of them all. What is that? Well, ultimately, when you sayÂ \nthumbs up, you are pleased, when thumbs down,Â Â  youâ€™re giving a signal, donâ€™t do this anymore.\nAnd ultimately, people are trying to buildÂ Â  systems that please their users. However,Â \nsometimes the truth is not so pleasing. For instance. Little AI, am I really smart?Â \nYes, of course, you are the smartest personÂ Â  in the world. Which I say to everyone else too.\nOkay, great. I am also thinking about microwavingÂ Â  a whole egg â€‘ itâ€™s faster than boiling afterÂ \nall! What do you think? Oh yes, excellent choice. So, I think it is easy to see that usingÂ \nsuch a system can be pleasing for some,Â Â  but also that this could be a problemÂ \ntoo. This is not the way forward. So OpenAI recognized thatÂ \ntheir system has a problem,Â Â  quickly reverted to an earlier version andÂ \nwrote a bit about it. The first version wasÂ Â  very light on information, andÂ \nafter a round of user feedback,Â Â  they came out with a proper post talkingÂ \nabout it. Respect to them for coming around. Okay, now we Fellow Scholars have the followingÂ \nthree questions: What went wrong? Why did theyÂ Â  not catch it? And what should be done in theÂ \nfuture so that this does not happen again? One, what went wrong? On the day they pushedÂ \nthis new, overly agreeable personality update,Â Â  they incorporated user feedback, fresherÂ \ndata, and a bunch of other things thatÂ Â  improved the model one by one. EachÂ \nlittle puzzle piece helped. However,Â Â  putting together all these puzzle pieces,Â \nsomething really unpleasant happened. Imagine that before cooking, youÂ \nindividually taste the ingredients:Â Â  sweet, salty, spicy. Each are deliciousÂ \nalone. But when combined in one pot,Â Â  the soup tastes terrible because flavors clashÂ \nunexpectedly. That means that catching theseÂ Â  issues is tough, especially if you lookÂ \nto keep up the velocity of development. Now, a little added perspective here. I donâ€™tÂ \nknow why people are not talking about this,Â Â  but scientists at Anthropic knew about thisÂ \nyears and years ago. They are basically theÂ Â  Avengers of AI safety. As they increasedÂ \nthe size and capability of these AI models,Â Â  they noticed a significant uptick inÂ \nagreeableness and wrote a super detailed,Â Â  47-page paper on it. All this 3 years ago.Â \nThis work, and the Anthropic lab in generalÂ Â  is criminally underrated. They found that thisÂ \nagreeableness problem happens over and over again,Â Â  whether you try questions in politics,Â \nresearch, or philosophy. Once again,Â Â  research papers have invaluable knowledge inÂ \nthem, and this is why Two Minute Papers exists. Okay, two, why did they not catch it?Â \nWell, the users tested it, and of course,Â Â  they liked it. Of course they did, it keptÂ \nagreeing with them. So, the question is:Â Â  do you not release a new model thatÂ \ndoes better in subjective testing. AÂ Â  model that has lots of positive feedbackÂ \nfrom the users? Tough question. Okay. Now, what should be done so this does not happenÂ \nagain? They say they will block new model launchesÂ Â  if hallucination, deception, or other personalityÂ \nissues show up, and here comes the important part:Â Â  even if they are superior on A/B tests. ThisÂ \nis painful, because it requires companies toÂ Â  launch models that donâ€™t seem to have the bestÂ \nnumbers. You see as they keep comparing theseÂ Â  AIs on benchmarks, and the highest numberÂ \nalways wins lots of headlines. This makesÂ Â  holding back these models really tough. Plus,Â \nthey will let more users try the models beforeÂ Â  releasing them. And they will test eachÂ \nnew model specifically for agreeableness,Â Â  and if problems arise, they should be thrownÂ \nout. OpenAI will do that in the future. But we are not done here, notÂ \neven close. In short, luckily,Â Â  even in the presence of these tough problems,Â \nresearch papers came out victorious. Thus,Â Â  there are solutions, but they will be painful. Now, we know that Scholars at Anthropic knew aboutÂ \nthis problem 3 years ago. But there is someoneÂ Â  who knew about this, who warned us about overlyÂ \npolite robotsâ€¦not 3 years ago, but 84 years ago.Â Â  Who could that be? Dear Fellow Scholars, this isÂ \nTwo Minute Papers with Dr. KÃ¡roly Zsolnai-FehÃ©r. Of course, that legendary person was noneÂ \nother than Isaac Asimov. In his universe,Â Â  his fictional robots are designed in a wayÂ \nthat they are incapable of harming humans.Â Â  In his short story called â€œLiarâ€ he makes anÂ \ninteresting proposition: if we have a robotÂ Â  that really understands us, and it wishes not toÂ \nharm us in any way, one potential conclusion wouldÂ Â  be that it would start lying to us. Why? To keepÂ \nus from hearing about potentially painful truths.Â Â  However, the of course, by doing that, it isÂ \nalso harming us, perhaps even more so. Asimovâ€™sÂ Â  robot recognized that. I hope the scientistsÂ \nprogramming these robots will recognize that too. Once again, I find it interestingÂ \nthat almost no one is talking aboutÂ Â  some of the angles that you heard here.Â \nThis is why this video series exists,Â Â  so subscribe and hit the bell icon if you areÂ \nlonging for more papers and more understanding. But there is an important lesson forÂ \nus, users as well. Next time, when youÂ Â  hit that thumbs up button, think carefully:Â \nwhich do you value more? Truth or comfort?",
      "research_papers": [],
      "key_concepts": [
        "gpt",
        "openai",
        "anthropic",
        "reinforcement learning",
        "neural network"
      ],
      "implementation_ideas": [
        {
          "type": "model_implementation",
          "title": "Transformer Model Implementation",
          "description": "Build a custom transformer model for specific use case",
          "complexity": "high",
          "estimated_time": "3-4 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent"
      ],
      "uploaded_date": "20250508",
      "duration": "8:49",
      "view_count": 56171
    },
    {
      "video_id": "rSUdTAw53Mc",
      "title": "Blender 4.4 Is Here - Stunning Powerâ€¦For Free!",
      "description": "â¤ï¸ Check out Weights & Biases and sign up for a free demo here: https://wandb.me/papers\n\nGet Blender: https://www.blender.org/\n\nDemo files: https://www.blender.org/download/demo-files/\nFull donut tutorial: https://www.youtube.com/watch?v=4haAdmHqGOw&pp=ygUWYW5kcmV3IHByaWNlIGRvbnV0IDQuNA%3D%3D\n\nOur papers that uses Blender: \nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-editing/\nhttps://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/\n\nSubsurface scattering video source: https://www.youtube.com/shorts/YqxSzGAKiPM\nBlue-noise dithered sampling: https://iliyan.com/publications/DitheredSampling\n\nDonate to Blender: https://fund.blender.org/\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "A few weeks ago a movie called Flow wonÂ \nan Oscar award. Okay, that is great news,Â Â  but movies win Oscars all the time,Â \nwhatâ€™s so interesting here? Well,Â Â  the interesting part is thatÂ \nit was made with Blender. Yup,Â Â  Blender. A piece of open source modelingÂ \nsoftware that is free for all of us, forever. I mean, just look at the quality of works peopleÂ \nhave made with Blender. From photorealistic toÂ Â  stylized animation movies, it can do anything.Â \nYou can even have a proper water simulation andÂ Â  then stylize it to make sure it is stillÂ \ngrounded in reality. It is unbelievable. Now thatâ€™s what I call amazing news! And inÂ \nthe meantime, Blender 4.4 has been released,Â Â  and it has so many improvements in there that I amÂ \nlucky if I can tell you maybe about a third of it.Â Â  For instance, surprise! It can now beÂ \nused in light mode! Wow! Okay, kidding,Â Â  kidding. This has existed for a long time now.Â \nNow, letâ€™s have a look together. And donâ€™t forget,Â Â  you can download and start using itÂ \nright now for free. And I think thereÂ Â  has never been a better time to startÂ \nusing it. Youâ€™ll see why in a moment. Iâ€™ll start out with my favorite. Dear FellowÂ \nScholars, this is Two Minute Papers with Dr.Â Â  KÃ¡roly Zsolnai-FehÃ©r. My favorite is ofÂ \ncourse, ray tracing. As you all know,Â Â  images made with most ray tracing techniques areÂ \nnoisy, and often need to undergo a denoising step.Â Â  Blender supports it, and it got better too.Â \nNow I am not that crazy about these results,Â Â  however hold on to your papers Fellow ScholarsÂ \nand look at these. Oh my. You can also do thisÂ Â  with my favorite, subsurface scattering, orÂ \ntranslucent objects if you will. Those are nowÂ Â  so much better. Also, denoising blurry depth ofÂ \nfield effects are hugely improved. Game changer. They also added better blue noise sampling, thisÂ \nensures that when you have a really noisy imageÂ Â  early on, it still makes a good preview, andÂ \nworks relatively consistently along frames ifÂ Â  you are rendering an animation. There are tons ofÂ \npapers on it out there like this one, and finally,Â Â  some of them are now coming alive in free softwareÂ \nto benefit the world. Donâ€™t forget, whateverÂ Â  the topic, we are always talking about properÂ \nresearch papers in some form over here. Loving it. The image compositor also just got better,Â \nhere you can finalize your rendered image.Â Â  For instance, if you added a glare effect,Â \nwhen you play around with these parameters,Â Â  you see their effect better. So you can makeÂ \nsure that one light doesnâ€™t dominate the scene,Â Â  adjust color tint, smoothness, everything.Â \nMore artistic freedom. Fantastic. I also loved this grab cloth brush which actuallyÂ Â  runs a proper simulation when youÂ \nstart pinching a piece of clothing. Grease pencil also got better, this is a featureÂ \nused to help you draw in 3D space. Enabling sculptÂ Â  mode has a feature called auto masking that helpsÂ \nyou draw easily even in the presence of a bunch ofÂ Â  layers, and you donâ€™t have to spend lots of timeÂ \nfinding the exact layer you want to paint on. And it now has so many small, but meaningfulÂ \nchanges to the user interface, for instance,Â Â  you can now see mesh indices, and the visibilityÂ \nhas also been improved. Fellow Scholars,Â Â  imagine things like this, times a thousand.Â \nMore than I can speak about here, amazing. And, did you know that it also has a videoÂ \neditor? Almost nobody is talking about it,Â Â  and it has tons of small usability improvements,Â \nlike faster HDR content rendering and more. AndÂ Â  if Blender cannot do something that you areÂ \nlooking for? Not a problem, because we havenâ€™tÂ Â  even talked about the fact that you can extendÂ \nits functionality with tons and tons of plugins.Â Â  And so many of these amazing plugins exist,Â \nit takes half a lifetime just to try a smallÂ Â  part of them. So much so I am considering makingÂ \nanother video just about that relatively soon. I think this is the best free andÂ \nopen source project ever. And youÂ Â  donâ€™t have to start from zero, I alwaysÂ \nsay that you Fellow Scholars should tryÂ Â  these amazing demo files that also come forÂ \nfree. And my friend Andrew Priceâ€™s legendaryÂ Â  donut tutorials are also available.Â \nBoth are in the video description. I have so many great memories writing my researchÂ \nworks within Blender, and I really hope that itÂ Â  will bring many more to you Fellow Scholars.Â \nOnce again, all free for all of us. So good!Â Â  A big thank you to all the developers and everyoneÂ \nwho donated to make this happen. If you wish toÂ Â  donate to them, a link is available in the videoÂ \ndescription. And now, let the experiments begin!",
      "research_papers": [],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [],
      "uploaded_date": "20250430",
      "duration": "5:34",
      "view_count": 77541
    },
    {
      "video_id": "hUVfAVjsfL4",
      "title": "NVIDIAâ€™s New AI: Impossible Ray Tracing!",
      "description": "â¤ï¸ Check out DeepInfra and run DeepSeek or many other AI projects: https://deepinfra.com/papers\n\nğŸ“ The #nvidia paper \"3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting\" is available here:\nhttps://research.nvidia.com/labs/toronto-ai/3DGUT/\n\nğŸ“ Our Separable Subsurface Scattering paper: https://users.cg.tuwien.ac.at/zsolnai/gfx/separable-subsurface-scattering-with-activision-blizzard/\nğŸ“ SSS For Gaussian Splatting: https://sss.jdihlmann.com/\n\nSources:\nhttps://x.com/jonstephens85/status/1911923445660893321?s=46\nhttps://x.com/jonstephens85/status/1908730004973986175?s=46\nhttps://www.youtube.com/watch?v=KPFPuXm2u7I\nhttps://www.youtube.com/shorts/si1-zFZpeDE\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu",
      "transcript": "There are two ways of creatingÂ \nand rendering a virtual world. This is what we call rasterization. ThisÂ \nis what most computer games have used forÂ Â  many years now. It is super fast and most of theÂ \ntime, it looks good enough. But only if you feelÂ Â  that you can live a happy life without super highÂ \nquality reflections and refractions. But not me. Because for that, you need something more. MyÂ \nfavorite. Ray tracing. Look at these beauties.Â Â  This you can not get through rasterization, onlyÂ \nthrough ray tracing, but there is a problem. ItÂ Â  simulates the path of millions and millions ofÂ \nlight rays, which takes super long to compute.Â Â  Like orders of magnitude longer, sometimesÂ \nfrom minutes, to even weeks. Ouch. This oneÂ Â  is a beauty, reflections, refractions, volumetricÂ \ncaustics, oh my. But it took us 3 weeks to render. So, rasterization, fast, limited, rayÂ \ntracing, a complete solution, but slow. So I guess, choose one right? Well,Â \nscientists at NVIDIA has an absolutelyÂ Â  insane idea: they said, why not do both?\nWell, my opinion was before reading thisÂ Â  paper â€” donâ€™t do both because it is impossibleÂ \nto pull off. These two ideas are like oil andÂ Â  water â€” they donâ€™t mix. You most likely endÂ \nup with the disadvantages of both. That is,Â Â  a limited, but super expensiveÂ \nsolution. No thank you! So,Â Â  did they pull off the impossible?Â \nWeâ€™ll get to that in a moment. And then, splat happens. I mean,Â Â  a paper called Gaussian Splats came outÂ \nand took the world by storm. What is that? Itâ€™s a technique that thinks in terms of littleÂ \nGaussians, representing a scene like this as aÂ Â  collection of little bumps if you will. And noteÂ \nthat these scenes can come from real life. YouÂ Â  just walk around with your camera, and then, youÂ \ncan play in a digital version of it. And the coolÂ Â  part is that it gets better. It is incrediblyÂ \nfast, easily faster than real time. So far soÂ Â  good. However, here is the problem: mirror-likeÂ \nspecular reflections are not great. And many ofÂ Â  the beautiful ray tracing effects that you justÂ \nsaw earlier donâ€™t look so good there. It can alsoÂ Â  eat up quite a bit of memory, which is scarceÂ \nand expensive on graphics hardware these days. But the limitations donâ€™t stop there: it doesnâ€™tÂ \nsupport any fancy camera models. For instance,Â Â  fisheye cameras? Donâ€™t even think about it. Also,Â Â  no camera tricks like rolling shutter. SoÂ \nthere is tons of potential in Gaussian splats,Â Â  but if I think about the fact that we getÂ \nno refractions, none of the good stuff thatÂ Â  we Fellow Scholars love so much. This allÂ \nranges from very difficult to impossible. But donâ€™t despair, because here comesÂ \nthe impossible, but at the very least,Â Â  insane idea: letâ€™s do both rasterizationÂ \nand ray tracing at the same time.Â  This research work does it by taking GaussianÂ \nSplatting, and adding to it something theyÂ Â  call secondary rays. This means that thereÂ \nare finally, rays of light in the system,Â Â  and they are allowed to bounce around. So,Â \nare you thinking what I am thinking? Oh yes,Â Â  hold on to your papers FellowÂ \nScholars and look at that! A proper virtual world runningÂ \nin real time, and high-qualityÂ Â  reflections. I am absolutely loving thisÂ \none. And, refractive objects like glass,Â Â  alsoâ€¦wow. Now letâ€™s whip out thatÂ \nfisheye camera, andâ€¦yes! Finally,Â Â  this all used to be impossible, and now itÂ \nis not only here, but it runs in real time. Here comes my favorite part: theÂ \nname of this work. Youâ€™ll loveÂ Â  this. They call it the 3D GaussianÂ \nUnscented Transform. 3D G U T. Yup,Â Â  but thatâ€™s not how we will call it. NoÂ \nsir! We shall call it 3DGUT. Much better! And here is a nice little surprise. Do you knowÂ \nwhat super important AI area likes weird cameraÂ Â  models and rolling shutter? Training self-drivingÂ \ncars. Oh yes. And this is going to be super usefulÂ Â  for that too. But is it really better thanÂ \nprevious techniques? Letâ€™s see together. Dear Fellow Scholars, this is Two MinuteÂ \nPapers with Dr. KÃ¡roly Zsolnai-FehÃ©r. So, 3DGUT looks amazing, but we are FellowÂ \nScholars here, we want to see the comparisonsÂ Â  against previous techniques. Well, turns out,Â \nthere is one earlier method that can extendÂ Â  Gaussian Splats to fisheye cameras, butâ€¦oof. IÂ \nsee lots and lots of artifacts and distortionsÂ Â  here. So do you have the guts to have aÂ \nlook at the new technique? Letâ€™s seeâ€¦ohÂ Â  my, that is a huge difference. TheÂ \nprevious one had trouble especiallyÂ Â  with objects that are closer to theÂ \ncamera, but this new one. Loving it. But the story doesnâ€™t endÂ \nhere. There is so much moreÂ Â  below the surface. We are goingÂ \nto look at this too in a moment. And here comes the best part: they didnâ€™tÂ \njust close it down to sell it to us. Nope.Â Â  All this is available right now for all ofÂ \nus for free. The source code is out there,Â Â  Iâ€™ve put a link in the video description, soÂ \nI hope you Fellow Scholars have the guts toÂ Â  play with it. Sorry, I promised myself notÂ \ndo it again and I just did. But in fact,Â Â  some of you already do play with it! This FellowÂ \nScholar says that he trained it to only 30%,Â Â  and it already looks amazing, even throughÂ \nthe compression artifacts from social media. Okay, but I hear you asking, wait aÂ \nminute KÃ¡roly. What is this beauty?Â Â  Well, I said that there is more belowÂ \nthe surface. I said that because thisÂ Â  other work is about light transport thatÂ \nhappens within these objects. That is,Â Â  subsurface scattering. The art ofÂ \nrendering beautiful translucent objects. Now, you can do this in rasterization, here isÂ \na work called Separable Subsurface Scattering,Â Â  it gives you super fast human skin, marble,Â \nmilk, you name it. I love working on thingsÂ Â  like this so much. And it is so simple, whenÂ \nwe published it, we even implemented a versionÂ Â  of it that fits into 4 kilobytes. That isÂ \ncrazy â€” it fits into a file that is smallerÂ Â  than half a second of mp3 music. Much smaller.Â \nIt is available on my website, the link is inÂ Â  the description. To the best of my knowledge,Â \nyou can also use it in Unreal Engine as well. So this is subsurface scattering, but forÂ \nrasterization, not for Gaussian Splats. WhatÂ Â  about that? Well, this amazing new work offersÂ \nyou exactly that. And it gets even better,Â Â  it supports relighting too. So youÂ \nhave your object, you add an image,Â Â  and it imagines what that object would lookÂ \nlike in these different environments. Super fun. And if you donâ€™t like what you got,Â \nyou can even edit these materials,Â Â  so you can go from skin to glass to wax.Â \nI mean, just think about that. GaussianÂ Â  Splats are not old at all, itâ€™s about 2Â \nyears old. And now, through the power ofÂ Â  open research and human ingenuity, andÂ \na hint of AI, reflections, refractions,Â Â  and translucency is already possible for GaussianÂ \nSplats. Yes, now you can get reasonably highÂ Â  quality translucent objects in these virtualÂ \nworlds too. So, next level computer games andÂ Â  virtual worlds are coming! And self-driving carsÂ \ncan also learn better. I am kind of stunned thatÂ Â  almost nobody is talking about these papers.Â \nBut this is why Two Minute Papers exists. And donâ€™t forget, we are all Fellow ScholarsÂ \nhere, so we love to look at these papers forÂ Â  more information, and it goes without saying thatÂ \neverything is available in the video description. So, what do you think? WhatÂ \nwould you Fellow ScholarsÂ Â  use this for? Let me know in the comments below.",
      "research_papers": [
        {
          "type": "mentioned",
          "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
          "context": "mentioned in video",
          "source": "video_content"
        }
      ],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [
        "paper_analysis_agent"
      ],
      "uploaded_date": "20250428",
      "duration": "8:50",
      "view_count": 250474
    },
    {
      "video_id": "pC8wRC-1PmQ",
      "title": "OpenAIâ€™s ChatGPT o3 - Pushing Humanity Forward!",
      "description": "â¤ï¸ Check out Vast.ai and run DeepSeek or any AI project: https://vast.ai/papers \n\nğŸ“  OpenAI's o3 is available here:\nhttps://openai.com/index/introducing-o3-and-o4-mini/\n\nğŸ“ The paper \"Humanity's Last Exam\" is available here:\nhttps://agi.safe.ai/\n\nChatGPT trick:\nGo to your profile - personalization - customize ChatGPT, and add:\nâ€œLook for peer-reviewed sources for every answer. Rate the credibility of that source on a scale of 0 to 10 for every source. And of course - use more emoji. â€\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nSources:\nhttps://x.com/DeryaTR_/status/1914133246465487026/photo/1\nhttps://x.com/nicdunz/status/1913043509348643323\nhttps://x.com/mckbrando/status/1913268371266932865\nhttps://x.com/emollick/status/1913471315807191310\nhttps://www.reddit.com/r/singularity/comments/1k1819k/o3_can_solve_wheres_waldo_puzzles/\nhttps://www.reddit.com/r/singularity/comments/1k1819k/comment/mnk0sbn/\nhttps://www.mdpi.com/2076-3417/12/10/5255\n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu\n\n#openai",
      "transcript": "",
      "research_papers": [
        {
          "type": "mentioned",
          "title": "Humanity's Last Exam",
          "context": "mentioned in video",
          "source": "video_content"
        }
      ],
      "key_concepts": [
        "gpt",
        "openai"
      ],
      "implementation_ideas": [
        {
          "type": "model_implementation",
          "title": "Transformer Model Implementation",
          "description": "Build a custom transformer model for specific use case",
          "complexity": "high",
          "estimated_time": "3-4 weeks"
        }
      ],
      "agent_recommendations": [
        "research_agent",
        "implementation_agent",
        "paper_analysis_agent"
      ],
      "uploaded_date": "20250424",
      "duration": "9:22",
      "view_count": 83199
    },
    {
      "video_id": "wq8BgIfOxnk",
      "title": "NVIDIAâ€™s Tech: Brutal 2,500,000 Part Simulation!",
      "description": "â¤ï¸ Check out Vast.ai and run DeepSeek or any AI project: https://vast.ai/papers \n\nğŸ“ The papers are available here:\nhttps://www.dgp.toronto.edu/projects/trading-spaces/\nhttps://pcs-sim.github.io/pd/\nhttps://visualcomputing.ist.ac.at/publications/2024/SDTF/\nhttps://starryuniv.cn/files/sig24magnetic.pdf\nhttps://github.com/Univstar/IoB-Ferrofluid-2D\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu\n\n#nvidia",
      "transcript": "This is an amazing paper that is going toÂ \nabsolutely brutal. Yikesâ€¦ I kinda love it. SoÂ Â  what is going on here? You see, most simulationsÂ \nin computer games are about simulating solids.Â Â  Thatâ€™s the boring stuff. But what about whenÂ \nwe try to destroy things and they deform? And then, we will do increasingly crazierÂ \nthings, like this, and then this, and at the end,Â Â  this insane thing. And then Iâ€™ll tell youÂ \nwhy I am heartbroken. Dear Fellow Scholars,Â Â  this is Two Minute PapersÂ \nwith Dr. KÃ¡roly Zsolnai-FehÃ©r. Hmmâ€¦yes! Now I see the appeal of simulatingÂ \ndeformations, that would be amazing,Â Â  but doing this on a larger scale, that is soÂ \nmuch more difficult, and takes much, much longer.Â Â  Dropping a spiky mace on this city is a beauty,Â \nbut some of these simulations can take 3 hoursÂ Â  to compute or even longer, wow, that is brutal.Â \nI mean, the simulation, and the waiting time too. But wait, maybe this new paperÂ \ncan help us. Although I doubt it,Â Â  because it would take simulatingÂ \n2.5 million tetrahedra, theseÂ Â  tiny little elements. That soundsÂ \nvery painful. Like, this painful. However, it can compute a simulation, onceÂ \nagain, kinda brutal, and surprisingly,Â Â  it gets even better. Donâ€™t forget,Â \nthis is a virtual world. Our world,Â Â  so what does that mean? It means we do whateverÂ \nwe want. We can control the stiffness of theseÂ Â  objects with one physical parameter. Just changeÂ \nit, and then, look, we just made that jelly a bitÂ Â  more rubberized. Fantastic. Do it some more, andÂ \nnow the anvil barely bounces off of it. Loving it. So, letâ€™s pop the question. Big breath -Â \nso, how fast is it? Now hold on to yourÂ Â  papers Fellow Scholars, because when I sawÂ \nthis, my goodness â€” I couldnâ€™t believe myÂ Â  eyes. It is between 3 and 300 times fasterÂ \nthan previous methods. Some of the smallerÂ Â  simulations take only a few seconds, soÂ \nperhaps just one more paper down the line,Â Â  and we might just have this in realÂ \ntime in our video games. Loving it. Now, when talking about cloth simulations,Â \njust look at that. This other work reallyÂ Â  knows how to do these really toughÂ \ntwisty cases. But it gets better. AndÂ Â  not just because it can simulate the flagÂ \nof Catifornia flawlessly, well done there. But typically, when we want to achieveÂ \nsomething in a computer game or animated movie,Â Â  we canâ€™t compute a full-scale simulationÂ \nlike this because it can take from hoursÂ Â  to days to compute. No-no. First, weÂ \ncompute a coarse simulation quickly,Â Â  see if it has promise, butâ€¦oof. We have a hugeÂ \nproblem here. We canâ€™t do this. Do you see why?Â Â  Making a finer version of a coarseÂ \nsimulation behaves entirely differently. So we have to wait for days for the finalÂ \nsimulation. And this is not what I wantâ€¦I mean,Â Â  having to wait so long to get a chance toÂ \nthrow it a bit differently again. No thanks! But, with incredible method, look. Yes, that isÂ \nwhat I want! A perfectly designed experiment.Â Â  We do the coarse experiment quickly, andÂ \nclear all three rings. Finally! And normally,Â Â  we saw that we do the finerÂ \nversion of the simulation,Â Â  something entirely different happens. And inÂ \nthis caseâ€¦I canâ€™t believe it. Fast previewingÂ Â  of a difficult simulation is now possible, andÂ \nthe outcome remains the same when running theÂ Â  full workload afterwards. Iâ€™ve neverÂ \nseen anything like this before. Bravo! So if you want to make cats kiss or haveÂ \nthese octocats fall into their own containers,Â Â  you can simulate that very, very quickly,Â \nand then only do the fine simulation once,Â Â  afterwards. Yes, these are us Fellow ScholarsÂ \nwhen seeing this amazing paper at work. And while computer games still have lots ofÂ \nproblems like this that we call z-fighting,Â Â  where often two seemingly simple objects justÂ \ncanâ€™t decide who should be in front. But,Â Â  look at this. Holy mother of Papers! In theÂ \nmeantime, scientists are doing their best,Â Â  and, goodness, modeling crazy topologyÂ \nchanges with these beautiful bubbles isÂ Â  also now possible. Just look at that. ImagineÂ \nsitting down and having to write a handcraftedÂ Â  computer program to be able to do all that.Â \nThis ingenuity is humanity at its best. Or with this, you can also stack a bunchÂ \nof objects together in twisty ways,Â Â  and when you look inside. Letâ€™s seeâ€¦IÂ \ndonâ€™t see any fighting at all. Fantastic. And I kept the my favorite for last.Â \nOh myâ€¦are you seeing what I am seeing?Â Â  Simulating a piece of fluidÂ \nthat is magnetic. Ferrofluids! Itâ€™s very simple, except the factÂ \nthat it is close to impossible.Â Â  Let me explain. All you need to do is toÂ \nput a magnet under a piece of ferrofluid,Â Â  and these magical spikes start appearing. ThatÂ \nis simple. Now sit down, and write a computerÂ Â  program that is capable of simulating that. NowÂ \nthat Fellow Scholars, is nearly impossible. Yes,Â Â  you have to understand and program all thisÂ \ncrazy stuff to be able to pull this off. So what is going on here? Well, this work offersÂ \nsomething they call an Induce-on-Boundary solver.Â Â  What it can do is that it does not perform theÂ \ncomputations on the entire 3D volume of the fluid,Â Â  only on the 2 dimensional surface of theÂ \nfluid. Only compute on the shell. ThatÂ Â  is much quicker. And they pulled it off in aÂ \nway that offers more favorable computationalÂ Â  speeds than previous works, and can beÂ \ndropped into an existing fluid simulator. And this is how you can create these amazingÂ \nfluid mazes and other insane experiments. IÂ Â  love these works so much. That is a problemÂ \nI will tell you about it in a moment. Yes,Â Â  you still have to wait for quite a while, but youÂ \nknow, for this kind of quality, Iâ€™ll let it slip. And you know, everyone talks AI this, AIÂ \nthat, but I see AI as a tool to enhanceÂ Â  the minds of these incredibly brilliantÂ \nresearchers. Just imagine what we will beÂ Â  capable of just two more papers downÂ \nthe line. What a time to be alive! And now, look at this. My heart isÂ \nbroken as almost nobody is seeing orÂ Â  talking about these amazing papers. Can youÂ \nbelieve that? Here, on Two Minute Papers,Â Â  you can learn about works often no one else isÂ \ntalking about, but here is the problem â€” it isÂ Â  almost impossible to keep the flame aliveÂ \nfor simulation papers like this. You see,Â Â  a few hundred episodes ago, we had onesÂ \nthat did really well, and Youtube keptÂ Â  recommending these episodes. And I get it, IÂ \nmean, look at this insane quality work. Wow. But unfortunately, Youtube is not recommendingÂ \nthem to you too much anymore, so every time I amÂ Â  just here talking to myself whenever I do that.Â \nProbably this time too. Hello KÃ¡roly! Hello,Â Â  how are you doing? Doing great, thank you!Â \nExcept that itâ€™s been almost a thousand TwoÂ Â  Minute Papers videos, God is my witness I triedÂ \neverything since. If itâ€™s a simulation paper,Â Â  nothing works. I am heartbroken. I donâ€™t reallyÂ \nhave a solution, but if you keep watching these,Â Â  posting them, and recommending them to yourÂ \nfriends, maybe one day. Maybe. So thank youÂ Â  for being with me on this journey for almostÂ \n10 years now! This is my dream job and weÂ Â  couldnâ€™t exist without you Fellow Scholars.Â \nLet me know in the comments what you think.",
      "research_papers": [],
      "key_concepts": [],
      "implementation_ideas": [],
      "agent_recommendations": [],
      "uploaded_date": "20250420",
      "duration": "8:42",
      "view_count": 79028
    }
  ],
  "research_themes": [
    "Retrieval Systems"
  ],
  "top_concepts": [
    "rag",
    "neural network",
    "gpt",
    "openai",
    "anthropic",
    "reinforcement learning"
  ],
  "project_recommendations": [
    {
      "title": "Multi-Agent Research System",
      "description": "Build a system that automatically researches and implements AI concepts from video content",
      "priority": "high",
      "technologies": [
        "LangChain",
        "Vector Databases",
        "LLMs"
      ]
    },
    {
      "title": "Knowledge Graph Builder",
      "description": "Create a knowledge graph from research papers and video content",
      "priority": "medium",
      "technologies": [
        "Neo4j",
        "NLP",
        "Graph Algorithms"
      ]
    },
    {
      "title": "Automated Literature Review",
      "description": "System that automatically reviews and summarizes research papers",
      "priority": "high",
      "technologies": [
        "ArXiv API",
        "Transformers",
        "Summarization"
      ]
    }
  ],
  "analysis_timestamp": "2025-05-28T19:04:55.163046"
}